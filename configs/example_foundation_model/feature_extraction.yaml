# Example: Feature Extraction with Foundation Model
#
# This config demonstrates PATTERN 1: Using a frozen foundation model to extract
# embeddings during preprocessing, then training a simple MLP decoder.
#
# Flow: Neural Data → [Frozen Foundation] → Embeddings → [Trainable MLP] → Predictions

# Model constructor (creates the MLP decoder)
model_constructor_name: example_foundation_mlp

# Config setter (sets dimensions based on foundation model)
config_setter_name: example_foundation_feature_extraction

# Model parameters
model_params:
  # Path to pretrained foundation model directory
  model_dir: example_foundation_model/pretrained_model

  # MLP decoder architecture (input_dim will be set automatically by config_setter)
  layer_sizes: [128, 50]  # Hidden layer(s) + output dimension
  dropout: 0.1
  use_layer_norm: true

# Training parameters
training_params:
  batch_size: 64
  epochs: 100
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping_patience: 10
  n_folds: 5

  # Loss and metrics
  losses: [mse]
  loss_weights: [1.0]
  metrics: [cosine_sim, nll_embedding]
  early_stopping_metric: cosine_sim
  smaller_is_better: false

# Data parameters
data_params:
  data_root: data
  word_column: word
  electrode_file_path: processed_data/all_subject_sig.csv
  embedding_type: gpt-2xl
  embedding_layer: 24
  embedding_pca_dim: 50
  subject_ids: [1, 2, 3]

  # Preprocessing function that extracts frozen features
  preprocessing_fn_name: example_foundation_feature_extraction

  # Preprocessor params (model_dir will be set by config_setter)
  preprocessor_params:
    batch_size: 32  # Batch size for feature extraction

# Trial identifier
trial_name: example_foundation_feature_extraction
