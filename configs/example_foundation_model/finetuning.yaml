config_setter_name: example_foundation_finetune
task_config:
  task_name: word_embedding_decoding_task
  data_params:
    data_root: data
    word_column: word
    subject_ids:
    - 1
    - 2
    - 3
  task_specific_config:
    embedding_type: gpt-2xl
    embedding_layer: 24
    embedding_pca_dim: 50
training_params:
  batch_size: 32
  epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.0001
  early_stopping_patience: 10
  n_folds: 5
  losses:
  - mse
  loss_weights:
  - 1.0
  metrics:
  - cosine_sim
  - nll_embedding
  early_stopping_metric: cosine_sim
  smaller_is_better: false
<<<<<<< HEAD
=======

# Data parameters
data_params:
  data_root: data
  word_column: word
  electrode_file_path: processed_data/all_subject_sig.csv
  embedding_type: gpt-2xl
  embedding_layer: 24
  embedding_pca_dim: 50
  subject_ids: [1, 2, 3]

  # No preprocessing function needed - raw data goes directly to model
  # window_width will be set automatically by config_setter

# Trial identifier
>>>>>>> 8e54602540327232ddbfc9ca795b4eab08e17474
trial_name: example_foundation_finetune
model_spec:
  constructor_name: example_foundation_finetune
  params:
    model_dir: example_foundation_model/pretrained_model
    mlp_layer_sizes:
    - 128
    output_dim: 50
    dropout: 0.1
    freeze_foundation: false
    num_frozen_layers: 2
  sub_models: {}
