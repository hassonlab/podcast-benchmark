model_constructor_name: population_transformer_mlp
config_setter_name: population_transformer
model_params:
  # These will be set by the config setter at runtime
  input_dim: 512  # PopulationTransformer embedding dimension
  output_dim: 50  # Word embedding dimension
  hidden_dims: [256, 128]  # Hidden layer dimensions for MLP decoder
training_params:
  batch_size: 32
  epochs: 100
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping_patience: 10
  n_folds: 5
  min_lag: -2000
  max_lag: 2000
  lag_step_size: 100
  fold_type: sequential_folds
  loss_name: cosine_similarity_loss
  metrics: [cosine_similarity_loss]
data_params:
  # Cross model data_params
  data_root: data
  embedding_type: gpt-2xl
  embedding_layer: 24
  embedding_pca_dim: 50
  window_width: 0.625
  preprocessing_fn_name: population_transformer_preprocessing_fn
  subject_ids: [9]
  # channel_reg_ex: LG[AB]*
  # PopulationTransformer specific config
  preprocessor_params:
    model_path: population_transformer/pretrained_weights/popt_brainbert_stft/pretrained_popt_brainbert_stft.pth
    batch_size: 32
    device: cuda
    use_cls_token: true
    pt_embedding_dim: 512
    resample_factor: 32
    window_width: 0.625
    # Model configuration for PopulationTransformer
    model_config:
      name: pt_model_custom
      position_encoding: multi_subj_position_encoding
      n_head: 8
      n_layers: 6
      hidden_dim: 512
      input_dim: 768
      layer_activation: gelu
      attention_weights: false
      use_token_cls_head: true
trial_name: population_transformer_base 