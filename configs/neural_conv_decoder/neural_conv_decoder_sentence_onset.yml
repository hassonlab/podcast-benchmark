model_constructor_name: ensemble_pitom_model
config_setter_name: neural_conv
task_name: sentence_onset_task
model_params:
  conv_filters: 128
  dropout: 0.2
  num_models: 10
  embedding_dim: 1 #this is output dimension of the decoder
  output_activation: sigmoid

training_params:
  batch_size: 32
  epochs: 50
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping_patience: 10
  n_folds: 5
  min_lag: -1000
  max_lag: 1000
  lag_step_size: 200
  # Use BCE on probabilities with model outputting sigmoid
  losses: [bce]
  metrics: [roc_auc,sensitivity, specificity, f1, precision]
  early_stopping_metric: f1
  smaller_is_better: false

data_params:
  data_root: data
  window_width: 0.625
  preprocessing_fn_name: window_average_neural_data
  electrode_file_path: processed_data/all_subject_sig.csv
  preprocessor_params:
    num_average_samples: 32
  # Task-specific knobs (optional)
  task_params:
    #this is the number of negative samples (no sentence onset) per positive sample (sentence onset)
    negatives_per_positive: 5
    #Negative examples are sampled within each sentence, at least `negative_margin_s` seconds after onset
    negative_margin_s: 0.75 #
    sentence_csv_path: processed_data/all_sentences_podcast.csv

trial_name: sentence_onset_lr

