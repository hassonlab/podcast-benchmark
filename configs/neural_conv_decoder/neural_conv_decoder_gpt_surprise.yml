model_constructor_name: ensemble_pitom_model
config_setter_name: neural_conv
task_name: gpt_surprise_task
model_params:
  conv_filters: 128
  reg: 0.35
  reg_head: 0
  dropout: 0.2
  num_models: 10
  embedding_dim: 1 #this is output dimension of the decoder
  output_activation: linear

training_params:
  batch_size: 32
  epochs: 50
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping_patience: 100
  n_folds: 10
  min_lag: 0
  max_lag: 400
  lag_step_size: 100
  # Use BCE on probabilities with model outputting sigmoid
  losses: [mse]
  metrics: [mse,corr]
  early_stopping_metric: mse
  smaller_is_better: false

data_params:
  data_root: data
  window_width: 0.625
  preprocessing_fn_name: preprocess_neural_data
  subject_ids: [9]
  # channel_reg_ex: ^G([1-9]|[1-5][0-9]|6[0-4])$
  preprocessor_params:
    num_average_samples: 32
  # Task-specific knobs (optional)
  task_params:
    
    content_noncontent_path: df_word_onset_with_pos_class.csv

trial_name: gpt_surprise
