model_constructor_name: ensemble_pitom_model
config_setter_name: neural_conv
task_name: placeholder_task
model_params:
  conv_filters: 128
  dropout: 0.2
  num_models: 10
  embedding_dim: 1
training_params:
  batch_size: 64
  epochs: 10
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping_patience: 10
  losses: [mse]
  metrics: ["cosine_sim"]
  early_stopping_metric: mse
  n_folds: 5
  min_lag: -2000
  max_lag: 1100
  lag_step_size: 200
  smaller_is_better: true
data_params:
  # Cross model data_params
  data_root: data
  word_column: word
  embedding_type: gpt-2xl
  embedding_layer: 24
  embedding_pca_dim: 50
  window_width: 1.0
  preprocessing_fn_name: window_average_neural_data
  subject_ids: [9]
  channel_reg_ex: ^G([1-9]|[1-5][0-9]|6[0-4])$
  # neural_conv_decoder specific config
  preprocessor_params:
    num_average_samples: 32
trial_name: ensemble_model_10_placeholder