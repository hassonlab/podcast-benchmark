model_constructor_name: foundation_model_finetune_mlp
config_setter_name: foundation_model_finetune_mlp
model_params: 
  mlp_layer_sizes: [50]
  drop_path: 0.0
  proj_drop: 0.0
  model_dir: /scratch/gpfs/zparis/ECoG-foundation-pretraining/checkpoints/model=patch_dim_2_small-grad_accum=2-encoder_mask_ratio=0.5_29115813/best_checkpoint
training_params:
  batch_size: 64
  epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.0001
  early_stopping_patience: 20
  n_folds: 5
  min_lag: -200
  max_lag: 800
  lag_step_size: 200
  losses: [nll_embedding]
  loss_weights: [1.0]
  metrics: ["cosine_sim", "mse"]
  early_stopping_metric: nll_embedding
  smaller_is_better: true
  grad_accumulation_steps: 4
data_params:
  # Cross-model fields
  data_root: data
  embedding_type: gpt-2xl
  embedding_layer: 24
  embedding_pca_dim: 50
  # Window width is set by config setter function to the model's sample length.
  window_width: -1
  preprocessing_fn_name: foundation_model_finetune_mlp
  subject_ids: [9]
  channel_reg_ex: ^G([1-9]|[1-5][0-9]|6[0-4])$
  # Model specific config.
  # preprocessor_params:
trial_name: "{}_finetune_loss={}_loss_weights={}_lr={}_wd={}_bs={}_grad_accum={}_minlag={}_maxlag={}_stepsize={}"
format_fields: [
    model_params.model_name,
    training_params.losses,
    training_params.loss_weights,
    training_params.learning_rate,
    training_params.weight_decay,
    training_params.batch_size,
    training_params.grad_accumulation_steps,
    training_params.min_lag,
    training_params.max_lag,
    training_params.lag_step_size,
  ]