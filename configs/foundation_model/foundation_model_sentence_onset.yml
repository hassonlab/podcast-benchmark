## Config for sentence onset decoding with foundation model finetune MLP head
model_constructor_name: foundation_model_finetune_mlp
config_setter_name: foundation_model_finetune_mlp
task_name: sentence_onset_task

model_params:
  # Final layer must output a single logit for binary classification
  mlp_layer_sizes: [1]
  freeze_foundation_model: true
  num_unfrozen_blocks: 1
  # Directory containing foundation checkpoint.pth and experiment_config.yml
  model_dir: foundation_model/models

training_params:
  batch_size: 32
  epochs: 60
  learning_rate: 0.0005
  weight_decay: 0.0001
  early_stopping_patience: 10
  n_folds: 5
  min_lag: -200
  max_lag: 400
  lag_step_size: 50
  losses: [bce]
  metrics: [roc_auc, f1]
  early_stopping_metric: roc_auc
  smaller_is_better: false

data_params:
  data_root: data
  # Window width is set by the config setter from the foundation model config
  window_width: -1
  preprocessing_fn_name: foundation_model_finetune_mlp
  subject_ids: [9]
  channel_reg_ex: ^G([1-9]|[1-5][0-9]|6[0-4])$
  task_params:
    negatives_per_positive: 5
    negative_margin_s: 0.75
    sentence_csv_path: data/stimuli/all_sentences_podcast.csv

trial_name: fm_sentence_onset_finetune
