{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd10f3c7-b558-4be5-a33e-8cb22026cfd1",
   "metadata": {},
   "source": [
    "# Training and evaluating decoding models with PyTorch\n",
    "\n",
    "Acknowledgments: This tutorial draws heavily on the encling tutorial by Samuel A. Nastase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79fc41-71ea-4d15-b4ad-d671b8d57cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this cell in colab\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install mne mne_bids himalaya scikit-learn pandas matplotlib nilearn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbca40a-2c26-4d60-91ef-945496f1448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mne_bids import BIDSPath\n",
    "\n",
    "from himalaya.backend import set_backend\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from decoding_utils import run_training_over_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52bf550-5d37-4478-a98e-233b48688d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    set_backend(\"torch_cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using cuda!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b31dd4-39be-424d-92ad-15f7c6394e86",
   "metadata": {},
   "source": [
    "## Loading features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c36cb0-2375-4fa2-9635-40e06bde4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_root = \"\"  # if using a local dataset, set this variable accordingly\n",
    "\n",
    "# Download the embedding, if required\n",
    "embedding_path = f\"{bids_root}stimuli/gpt2-xl/features.hdf5\"\n",
    "if not len(bids_root):\n",
    "    !wget -nc https://s3.amazonaws.com/openneuro.org/ds005574/$embedding_path\n",
    "    embedding_path = \"features.hdf5\"\n",
    "embedding_path = \"features.hdf5\"\n",
    "\n",
    "print(f\"Using embedding file path: {embedding_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adaf69d-5e84-46bf-82c8-2e4f26cf52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname, layer = 'gpt2-xl', 24\n",
    "with h5py.File(embedding_path, \"r\") as f:\n",
    "    contextual_embeddings = f[f\"layer-{layer}\"][...]\n",
    "print(f\"LLM embedding matrix has shape: {contextual_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b5698-5362-4aae-bdcb-582a382fb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the transcript, if required\n",
    "transcript_path = f\"{bids_root}stimuli/gpt2-xl/transcript.tsv\"\n",
    "if not len(bids_root):\n",
    "    !wget -nc https://s3.amazonaws.com/openneuro.org/ds005574/$transcript_path\n",
    "    transcript_path = \"transcript.tsv\"\n",
    "\n",
    "# Load transcript\n",
    "df_contextual = pd.read_csv(transcript_path, sep=\"\\t\", index_col=0)\n",
    "if \"rank\" in df_contextual.columns:\n",
    "    model_acc = (df_contextual[\"rank\"] == 0).mean()\n",
    "    print(f\"Model accuracy: {model_acc*100:.3f}%\")\n",
    "\n",
    "df_contextual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430f412-1e5c-4862-b98a-72a84c8ddf3a",
   "metadata": {},
   "source": [
    "When we extracted features, some words are split into separate tokens. Since we only have information of start and end for words, we will align the features from tokens to words for decoding models. Here, we simply average the token features across the same word. Now the features should be a numpy array with a shape of (number of words * feature dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74798d02-1eb4-4860-8952-4e7a5439d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_embeddings = []\n",
    "for _, group in df_contextual.groupby(\"word_idx\"): # group by word index\n",
    "    indices = group.index.to_numpy()\n",
    "    average_emb = contextual_embeddings[indices].mean(0) # average features\n",
    "    aligned_embeddings.append(average_emb)\n",
    "aligned_embeddings = np.stack(aligned_embeddings)\n",
    "pca = PCA(n_components=50, svd_solver='auto')\n",
    "pca_embeddings = pca.fit_transform(aligned_embeddings.tolist())\n",
    "print(f\"LLM embeddings matrix has shape: {aligned_embeddings.shape}\")\n",
    "print(f\"PCA embeddings matrix has shape: {pca_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3ac4c-2e9b-4f5f-a6bd-5abee2065cf2",
   "metadata": {},
   "source": [
    "We will also construct a dataframe containing words with their start and end timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dee81e-5ec3-41a8-a78a-7763486797f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word = df_contextual.groupby(\"word_idx\").agg(dict(word=\"first\", start=\"first\", end=\"last\"))\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb64f0-c4df-463b-8822-e7939432369e",
   "metadata": {},
   "source": [
    "## Loading brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288c9c9-8fbc-4859-9e81-ec4ac4b7591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = BIDSPath(root=f\"{bids_root}derivatives/ecogprep\",\n",
    "                    subject=\"03\", task=\"podcast\", datatype=\"ieeg\", description=\"highgamma\",\n",
    "                    suffix=\"ieeg\", extension=\".fif\")\n",
    "print(f\"File path within the dataset: {file_path}\")\n",
    "\n",
    "# You only need to run this if using Colab (i.e. if you did not set bids_root to a local directory)\n",
    "if not len(bids_root):\n",
    "    !wget -nc https://s3.amazonaws.com/openneuro.org/ds005574/$file_path\n",
    "    file_path = file_path.basename\n",
    "\n",
    "raw = mne.io.read_raw_fif(file_path, verbose=False)\n",
    "picks = mne.pick_channels_regexp(raw.ch_names, \"LG[AB]*\")\n",
    "raw = raw.pick(picks)\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf6369-f5f5-4e08-905c-41160ac322a2",
   "metadata": {},
   "source": [
    "We'll setup GloVe embeddings as well for comparison to GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0417d9-5059-4c24-be93-a1e1aefd8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('glove'):\n",
    "    !mkdir -p glove\n",
    "    !mkdir -p glove\n",
    "\n",
    "if not os.path.exists('glove/glove.6B.zip'):\n",
    "    # Wikipedia 2014 + Gigaword 5 (6B tokens, 50d, 100d, 200d, & 300d vectors)\n",
    "    !wget https://nlp.stanford.edu/data/glove.6B.zip -P glove/\n",
    "    \n",
    "    # Twitter (27B tokens, 25d, 50d, 100d, & 200d vectors)\n",
    "    # wget https://nlp.stanford.edu/data/glove.twitter.27B.zip -P glove/\n",
    "    \n",
    "    # Extract the downloaded zip file\n",
    "    !unzip glove/glove.6B.zip -d glove/\n",
    "\n",
    "glove_file = 'glove/glove.6B.50d.txt'\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70273863-ece2-4e0c-a1be-4fdc3c6c98a7",
   "metadata": {},
   "source": [
    "### Decoding Straight from Neural Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5631e-9189-4687-9702-f4aa3342c845",
   "metadata": {},
   "source": [
    "Here we decode straight from the neural data using a convolutional network per [https://www.nature.com/articles/s41593-022-01026-4]. To adapt this to your custom model for decoding you need two primary pieces.\n",
    "\n",
    "1. Define a decoding model and a constructor function.\n",
    "2. Define a data preprocessing function for preparing the data for your decoding model.\n",
    "\n",
    "Below I demonstrate how to do this with the raw neural data. In the Decoding Foundation Model section I demonstrate how to adapt this to a model's output which will be useful if you're trying to decode from something like BrainBert output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826232a2-73f2-4069-b48c-e2de4954bc1f",
   "metadata": {},
   "source": [
    "First we need to define a decoding model. This model is adapted from the 2021 paper linked above and is essentially a convolutional model. Below that I define an ensemble model which averages the outputs of several of these models which improves performance and was used in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5aba6-1acd-41f0-88cd-eb40b40bd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitomModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_dim,\n",
    "        conv_filters=128,\n",
    "        reg=0.35,\n",
    "        reg_head=0,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        PyTorch implementation of the PITOM decoding model.\n",
    "        \n",
    "        Args:\n",
    "            input_channels: Numbr of electrodes in data (int)\n",
    "            output_dim: Dimension of output vector (int)\n",
    "            conv_filters: Number of convolutional filters (default: 128)\n",
    "            reg: L2 regularization factor for convolutional layers (default: 0.35)\n",
    "            reg_head: L2 regularization factor for dense head (default: 0)\n",
    "            dropout: Dropout rate (default: 0.2)\n",
    "        \"\"\"\n",
    "        super(PitomModel, self).__init__()\n",
    "        \n",
    "        self.conv_filters = conv_filters\n",
    "        self.reg = reg\n",
    "        self.reg_head = reg_head\n",
    "        self.dropout = dropout\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define the CNN architecture\n",
    "        self.desc = [(conv_filters, 3), ('max', 2), (conv_filters, 2)]\n",
    "        \n",
    "        # Build the layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i, (filters, kernel_size) in enumerate(self.desc):\n",
    "            if filters == 'max':\n",
    "                self.layers.append(\n",
    "                    nn.MaxPool1d(kernel_size=kernel_size, stride=kernel_size, padding=kernel_size//2)\n",
    "                )\n",
    "            else:\n",
    "                # Conv block\n",
    "                conv = nn.Conv1d(\n",
    "                    in_channels=input_channels if i == 0 else conv_filters,\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=0,  # 'valid' in Keras\n",
    "                    bias=False\n",
    "                )\n",
    "                \n",
    "                # Apply weight decay equivalent to L2 regularization\n",
    "                self.layers.append(conv)\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.BatchNorm1d(filters))\n",
    "                self.layers.append(nn.Dropout(dropout))\n",
    "                \n",
    "                input_channels = filters\n",
    "        \n",
    "        # Final locally connected layer (using Conv1d with groups as approximation)\n",
    "        # Note: True locally connected layers aren't standard in PyTorch\n",
    "        # This is an approximation that would need to be customized further for exact equivalence\n",
    "        self.final_conv = nn.Conv1d(\n",
    "            in_channels=conv_filters,\n",
    "            out_channels=conv_filters,\n",
    "            kernel_size=2,\n",
    "            stride=1,\n",
    "            padding=0,  # 'valid' in Keras\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        self.final_bn = nn.BatchNorm1d(conv_filters)\n",
    "        self.final_act = nn.ReLU()\n",
    "        \n",
    "        # Output layer\n",
    "        self.dense = nn.Linear(conv_filters, output_dim)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply final conv block\n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_bn(x)\n",
    "        x = self.final_act(x)\n",
    "        \n",
    "        # Global max pooling\n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze(-1)\n",
    "        \n",
    "        # Apply output layer if needed\n",
    "        x = self.dense(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.tanh(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class EnsemblePitomModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_models: int,\n",
    "        input_channels,\n",
    "        output_dim: int,\n",
    "        conv_filters=128,\n",
    "        reg=0.35,\n",
    "        reg_head=0,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        PyTorch implementation of the PITOM decoding model.\n",
    "        \n",
    "        Args:\n",
    "            num_models: The number of models to include in the ensemble. The outputs will be averaged at the end.\n",
    "            input_channels: Numbr of electrodes in data (int)\n",
    "            output_dim: Dimensionality of output (int)\n",
    "            conv_filters: Number of convolutional filters (default: 128)\n",
    "            reg: L2 regularization factor for convolutional layers (default: 0.35)\n",
    "            reg_head: L2 regularization factor for dense head (default: 0)\n",
    "            dropout: Dropout rate (default: 0.2)\n",
    "        \"\"\"\n",
    "        super(EnsemblePitomModel, self).__init__()\n",
    "\n",
    "        self.models = nn.ModuleList()\n",
    "        for _ in range(num_models):\n",
    "            self.models.append(PitomModel(\n",
    "                input_channels,\n",
    "                output_dim,\n",
    "                conv_filters=conv_filters,\n",
    "                reg=reg,\n",
    "                reg_head=reg_head,\n",
    "                dropout=dropout\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run all models and average together all embeddings.\n",
    "        embeddings = torch.stack([model(x) for model in self.models])\n",
    "        return embeddings.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed2d89-6789-4ff8-8c08-e93098cbf50c",
   "metadata": {},
   "source": [
    "Our training function needs a constructor function which takes in a dictionary of model_params which you provide when you call the training function and returns a model object. This can be very simple as you see here.\n",
    "\n",
    "The reason I'm using a constructor here is that it allows us to extend our existing code to any decoding model we desire without having to change our code for every use case. Now the code for a BrainBert decoder can exist separate from the neural decoder and any other model we choose to use, but they all still use the same backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a190b34-4f17-4194-a240-1ae431b86936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitom_model(model_params):\n",
    "    return PitomModel(\n",
    "            input_channels=model_params['input_channels'],\n",
    "            output_dim=model_params['embedding_dim'],\n",
    "            conv_filters=model_params['conv_filters'],\n",
    "            reg=model_params['reg'],\n",
    "            reg_head=model_params['reg_head'],\n",
    "            dropout=model_params['dropout']\n",
    "        )\n",
    "\n",
    "def ensemble_pitom_model(model_params):\n",
    "    return EnsemblePitomModel(\n",
    "            num_models=model_params['num_models'],\n",
    "            input_channels=model_params['input_channels'],\n",
    "            output_dim=model_params['embedding_dim'],\n",
    "            conv_filters=model_params['conv_filters'],\n",
    "            reg=model_params['reg'],\n",
    "            reg_head=model_params['reg_head'],\n",
    "            dropout=model_params['dropout']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e368b-cff3-4ecc-a27c-231d58a9ad8d",
   "metadata": {},
   "source": [
    "Next we need a preprocessing function to transform the neural data as we would like for this particular model. Here I want to average over windows of 32 data points to essentially resample the data from 512 hz to 16 hz. \n",
    "\n",
    "The preprocessing function will always be passed data in the shape (num_words, num_electrodes, num_timesteps). num_words is defined by the df_word dataframe passed into data_params below, num_electrodes is the number of electrodes in the mne.Raw object passed into data_params, and num_timesteps is defined by the window_width * sampling_frequency of your mne.Raw. window_width is passed into data_params below in seconds. The preprocessing function requires that the returned data has shape [num_words, ...] where '...' is any arbitrary shape you need for your purposes. The only requirement is that data.shape[0] == num_words.\n",
    "\n",
    "Like the model constructor, we use this preprocessing function as an input to our training code because it doesn't require us to change the backbone code for every use case. Now all of our use cases can define their custom code in a logical place without having to change our backbone code and potentially lead to bloated functions which are prone to bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1406f-3d12-477b-a829-48a288eca5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_neural_data(data):\n",
    "    return data.reshape(data.shape[0], data.shape[1], -1, 32).mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f0e20-c873-47f4-8ad5-1c6c9ceaa2c7",
   "metadata": {},
   "source": [
    "With that we can now train a decoding model. Results will be written to your specified output directory. Currently we only write out the weighted_roc's of the model for the specified lags but you can expand on the existing code to write out new metrics as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8bc96c-b064-4b91-9f35-52c136f6ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = np.arange(-900, 1000, 100)\n",
    "\n",
    "weighted_roc_means = run_training_over_lags(lags, ensemble_pitom_model,\n",
    "        model_params={\n",
    "            'conv_filters': 128,\n",
    "            'reg': 0.35,\n",
    "            'reg_head': 0,\n",
    "            'dropout': 0.2,\n",
    "            'num_models': 10,\n",
    "            'input_channels': len(raw.ch_names),\n",
    "            'embedding_dim': pca_embeddings.shape[1],\n",
    "        },\n",
    "        training_params={\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 0.0001,\n",
    "            'early_stopping_patience': 10,\n",
    "            'n_folds': 5\n",
    "        },\n",
    "        data_params={\n",
    "            'raw': raw,\n",
    "            'df_word': df_word,\n",
    "            'word_embeddings': pca_embeddings,\n",
    "            'window_width': 0.625,\n",
    "            'preprocessing_fn': preprocess_neural_data,\n",
    "        },\n",
    "        trial_name='ensemble_model_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e707b3b-e95c-4852-979c-2990f759d5ac",
   "metadata": {},
   "source": [
    "One other thing you may want to do is decode into different sets of embeddings. Here we create arbitrary embeddings with no semantic information encoded. All we have to do to train over them is pass the embeddings of shape [num_words, embedding_dim] into our data_params. num_words must be the same as the number of words in the passed df_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bb206-f20d-44e6-8819-0a0b6c95b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate arbitrary embeddings \n",
    "words = df_word.word.tolist()\n",
    "unique_words = list(set(words))\n",
    "word_to_idx = {}\n",
    "for i, word in enumerate(words):\n",
    "    if word not in word_to_idx:\n",
    "        word_to_idx[word] = []\n",
    "    word_to_idx[word].append(i)\n",
    "    \n",
    "arbitrary_embeddings_per_word = np.random.uniform(low=-1.0, high=1.0, size=(len(unique_words), 50))\n",
    "arbitrary_embeddings = np.zeros((len(words), 50))\n",
    "for i, word in enumerate(unique_words):\n",
    "    for idx in word_to_idx[word]:\n",
    "        arbitrary_embeddings[idx] = arbitrary_embeddings_per_word[i]\n",
    "\n",
    "lags = np.arange(-900, 1000, 100)\n",
    "weighted_roc_means_arbitrary = run_training_over_lags(lags, ensemble_pitom_model,\n",
    "        model_params={\n",
    "            'conv_filters': 128,\n",
    "            'reg': 0.35,\n",
    "            'reg_head': 0,\n",
    "            'dropout': 0.2,\n",
    "            'num_models': 10,\n",
    "            'input_channels': len(raw.ch_names),\n",
    "            'embedding_dim': arbitrary_embeddings.shape[1],\n",
    "        },\n",
    "        training_params={\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 0.0001,\n",
    "            'early_stopping_patience': 10,\n",
    "            'n_folds': 5\n",
    "        },\n",
    "        data_params={\n",
    "            'raw': raw,\n",
    "            'df_word': df_word,\n",
    "            'word_embeddings': arbitrary_embeddings,\n",
    "            'window_width': 0.625,\n",
    "            'preprocessing_fn': preprocess_neural_data,\n",
    "        },\n",
    "        trial_name='ensemble_model_arbitrary_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729e3c7-95fb-4131-9556-46f1a8c60a23",
   "metadata": {},
   "source": [
    "We can do the same with GloVe embeddings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89095a0-58de-49f5-80f1-2f7284a064c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    # Convert to lowercase\n",
    "    word = word.lower()\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "words = df_word.word.tolist()\n",
    "preprocessed_words = [preprocess_word(word) for word in words]\n",
    "in_glove = []\n",
    "glove_embeddings = []\n",
    "\n",
    "for i, word in enumerate(preprocessed_words):\n",
    "    if word in glove_vectors:\n",
    "        glove_embeddings.append(glove_vectors[word])\n",
    "        in_glove.append(True)\n",
    "    else:\n",
    "        in_glove.append(False)\n",
    "\n",
    "df_word['in_glove'] = in_glove\n",
    "\n",
    "glove_embeddings = np.vstack(glove_embeddings)\n",
    "\n",
    "lags = np.arange(-900, 1000, 100)\n",
    "weighted_roc_means_glove = run_training_over_lags(lags, ensemble_pitom_model,\n",
    "        model_params={\n",
    "            'conv_filters': 128,\n",
    "            'reg': 0.35,\n",
    "            'reg_head': 0,\n",
    "            'dropout': 0.2,\n",
    "            'num_models': 10,\n",
    "            'input_channels': len(raw.ch_names),\n",
    "            'embedding_dim': glove_embeddings.shape[1],\n",
    "        },\n",
    "        training_params={\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 0.0001,\n",
    "            'early_stopping_patience': 10,\n",
    "            'n_folds': 5\n",
    "        },\n",
    "        data_params={\n",
    "            'raw': raw,\n",
    "            'df_word': df_word[df_word['in_glove']],\n",
    "            'word_embeddings': glove_embeddings,\n",
    "            'window_width': 0.625,\n",
    "            'preprocessing_fn': preprocess_neural_data,\n",
    "        },\n",
    "        trial_name='ensemble_model_glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56573870-fc97-4299-9d64-6ae2ebb64b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = pd.read_csv(\"results/ensemble_model_10_roc_means.csv\")\n",
    "arbitrary_data = pd.read_csv(\"results/ensemble_model_arbitrary_embeddings_roc_means.csv\")\n",
    "glove_data = pd.read_csv(\"results/ensemble_model_glove_roc_means.csv\")\n",
    "plt.plot(gpt_data.lags, gpt_data.rocs, label='GPT-2')\n",
    "plt.plot(arbitrary_data.lags, arbitrary_data.rocs, label='Arbitrary Embeddings')\n",
    "plt.plot(glove_data.lags, glove_data.rocs, label='GloVe')\n",
    "plt.axvline(0, color='red', alpha=0.5)\n",
    "plt.xlabel('Lags (ms)')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.title('AUC-ROC as a function of Lags')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d292786-ac45-4a5c-8a0b-59405c013b97",
   "metadata": {},
   "source": [
    "### Decoding from Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49515ab7-724b-4b80-8abc-22c91261c22d",
   "metadata": {},
   "source": [
    "Here we can go through the same process but now we want to decode from the outputs of our foundation model. The model I've supplied here is essentially randomly initialized so performance will not be good.\n",
    "\n",
    "First we'll load data for the subject we pretrain over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3fc44-3b7d-4488-98f3-0c3fb3bc3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = BIDSPath(root=f\"{bids_root}derivatives/ecogprep\",\n",
    "                    subject=\"09\", task=\"podcast\", datatype=\"ieeg\", description=\"highgamma\",\n",
    "                    suffix=\"ieeg\", extension=\".fif\")\n",
    "print(f\"File path within the dataset: {file_path}\")\n",
    "\n",
    "# You only need to run this if using Colab (i.e. if you did not set bids_root to a local directory)\n",
    "if not len(bids_root):\n",
    "    !wget -nc https://s3.amazonaws.com/openneuro.org/ds005574/$file_path\n",
    "    file_path = file_path.basename\n",
    "\n",
    "raw = mne.io.read_raw_fif(file_path, verbose=False)\n",
    "# picks = mne.pick_channels_regexp(raw.ch_names, \"LG[AB]*\")\n",
    "grid_ch_names = []\n",
    "for i in range(64):\n",
    "    channel = \"G\" + str(i + 1)\n",
    "    if np.isin(channel, raw.info.ch_names):\n",
    "        grid_ch_names.append(channel)\n",
    "\n",
    "\n",
    "raw = raw.pick(grid_ch_names,)\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabaa2e-d328-47e6-b466-b8f93f85f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from foundation_model.models_mae import MaskedAutoencoderViT\n",
    "from foundation_model.config import create_video_mae_experiment_config_from_file\n",
    "from foundation_model.utils import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f5927-1194-4bfd-9797-5a11f4b556f7",
   "metadata": {},
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68eae6-1e2b-4956-b460-823518c4b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the model and set it in eval mode.\n",
    "model_dir = \"foundation_model/models\"\n",
    "ecog_config = create_video_mae_experiment_config_from_file(os.path.join(model_dir, \"experiment_config.ini\"))\n",
    "\n",
    "model = create_model(ecog_config)\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"model.pth\"), weights_only=True))\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02fea1-0b9c-40af-a8d4-cdd492e332df",
   "metadata": {},
   "source": [
    "Now this is the important part for decoding. As above we have the same steps to accomplish:\n",
    "\n",
    "1. Define a decoding model and a constructor function.\n",
    "2. Define a data preprocessing function for preparing the data for your decoding model.\n",
    "\n",
    "First we'll define a decoding model. Because the model currently only outputs a 16 dimensional embedding of the neural data, we'll instead just use an MLP to go from our neural embedding to the 50 dimensional word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d581c36-5816-499c-957b-a30a14adbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=F.relu, dropout_rate=0.2, use_layer_norm=True):\n",
    "        \"\"\"\n",
    "        Initialize a Multi-Layer Perceptron with configurable architecture and LayerNorm.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes (list): List of integers specifying the size of each layer.\n",
    "                               First element is input size, last element is output size.\n",
    "            activation (function): Activation function to use between layers (default: ReLU).\n",
    "            dropout_rate (float): Dropout probability for regularization (default: 0.2).\n",
    "            use_layer_norm (bool): Whether to use LayerNorm after each hidden layer (default: True).\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        if len(layer_sizes) < 2:\n",
    "            raise ValueError(\"layer_sizes must contain at least input and output sizes\")\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList() if use_layer_norm else None\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        \n",
    "        # Create linear layers and layer norms based on specified sizes\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            \n",
    "            # Add layer norm for all but the output layer\n",
    "            if use_layer_norm:\n",
    "                self.layer_norms.append(nn.LayerNorm(layer_sizes[i+1]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, layer_sizes[0]]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [batch_size, layer_sizes[-1]]\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "\n",
    "            # Final layer will be normed.\n",
    "            if self.use_layer_norm:\n",
    "                x = self.layer_norms[i](x)\n",
    "            \n",
    "            # Apply activation, and dropout to all but the final layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03aa71-9a9e-4cfd-80ec-3de3039394ff",
   "metadata": {},
   "source": [
    "Now we can define our very simple constructor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92ef51-a741-43af-b190-9b13217aa5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_fn(model_params):\n",
    "    return MLP(model_params['layer_sizes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716a356-8073-406a-8965-19a84c610e6f",
   "metadata": {},
   "source": [
    "Here we can define our preprocessing function. As above it takes in data of shape (num_words, num_electrodes, num_timesteps). First I reshape the data into the shape expected by our foundation model (num_words, num_frequency_bands, resampled_time_steps, 8, 8) for our grid data. I then pass the data through the foundation model in batches to get data of shape (num_words, embedding_dim). This is now ready to be sent to my decoding model so I return that data.\n",
    "\n",
    "If you're decoding from something like BrainBert you'd likely want your preprocessing function to look something like this where you\n",
    "\n",
    "1. Get the data into the expected shape and sampling rate.\n",
    "2. Feed the data through the model in batches to reduce memory overhead.\n",
    "3. Combined embeddings in any way you see fit and then return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d4cf1-adb0-4774-ad4c-00da15eeceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foundation_model_preprocessing_fn(data):\n",
    "    data_config = ecog_config.ecog_data_config\n",
    "    data = data.reshape(data.shape[0], data.shape[1], -1, data_config.original_fs // data_config.new_fs)\n",
    "    data = data.mean(-1)\n",
    "    \n",
    "    for i in range(64):\n",
    "        channel = \"G\" + str(i + 1)\n",
    "        if not np.isin(channel, raw.info.ch_names):\n",
    "            data = np.insert(data, i, np.zeros_like(data[:, i, :]), axis=1)\n",
    "\n",
    "    # Reshape to [num_examples, frequency bands (currrently 1), time, num_electrodes]\n",
    "    data = np.einsum('bet->bte', data).reshape(data.shape[0], data.shape[2], 8, 8)\n",
    "    data = np.expand_dims(data, axis=1)\n",
    "\n",
    "    # Construct input dataset\n",
    "    batch_size = 16\n",
    "    foundation_embeddings = []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(data), batch_size)):\n",
    "            batch = torch.tensor(data[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            batch_embeddings = model(batch, forward_features=True)  # Shape: [batch_size, 16]\n",
    "            foundation_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    foundation_embeddings = np.vstack(foundation_embeddings)\n",
    "\n",
    "    return foundation_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c55484-6a12-4e00-86c1-d830d711fa14",
   "metadata": {},
   "source": [
    "We can decode with essentially the same setup as before with minimal changes. I just have to change a few parameters to fit the expectations of my foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c5737-1818-406c-83da-95a32e169cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = np.arange(-900, 1000, 100)\n",
    "\n",
    "weighted_roc_means = run_training_over_lags(lags, mlp_fn,\n",
    "        model_params={\n",
    "            'layer_sizes': [16, 32, 50],\n",
    "        },\n",
    "        training_params={\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'learning_rate': 0.001,\n",
    "            'weight_decay': 0.0001,\n",
    "            'early_stopping_patience': 10,\n",
    "            'n_folds': 5\n",
    "        },\n",
    "        data_params={\n",
    "            'raw': raw,\n",
    "            'df_word': df_word,\n",
    "            'word_embeddings': pca_embeddings,\n",
    "            'window_width': ecog_config.ecog_data_config.sample_length,\n",
    "            'preprocessing_fn': foundation_model_preprocessing_fn,\n",
    "        },\n",
    "        trial_name='foundation_model_trial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859edaf-e2fd-412a-a7a4-2052f1737cd8",
   "metadata": {},
   "source": [
    "Here we can see that it performs not well as expected with a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b673f1-c654-47e2-9e13-7f3b0fcbcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"foundation_model_trial.csv\")\n",
    "lags = data.lags\n",
    "weighted_roc_means = data.rocs\n",
    "plt.plot(lags, weighted_roc_means, label='GPT-2')\n",
    "# plt.plot(lags, weighted_roc_means_arbitrary, label='Arbitrary Embeddings')\n",
    "plt.axvline(0, color='red', alpha=0.5)\n",
    "plt.xlabel('Lags (ms)')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.title('AUC-ROC as a function of Lags')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb91620-c2d8-4dca-9bbd-1f80d1394823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog [~/.conda/envs/ecog/]",
   "language": "python",
   "name": "conda_ecog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
