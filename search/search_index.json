{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Podcast Benchmark Documentation","text":"<p>A benchmarking framework for neural decoding from podcast listening data. </p>"},{"location":"#decoding-tasks","title":"Decoding Tasks","text":"<ol> <li>Brain --&gt; perceived word decoding Translate brain signals to perceived words, comparing performance to previously published results.</li> <li>Audio Reconstruction Reconstruct podcast audio envelope from brain signal (Regression)</li> <li>Sentence Onset Detection Classify (binary) segments of brain data as containing the beginning of a sentence or not</li> <li>Content/Non-Content Words Classification (Binary classification)</li> <li>Part of Speech Classification (Multiclass classification)</li> <li>LLM Surprise Predict how likely the perceived word is given it's context (Regression)</li> <li>LLM Decoding Encode brain data as vector input to language models (GPT-2) for direct brain-to-text generation</li> </ol>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quickstart - Get up and running quickly</li> <li>Onboarding a New Model - Step-by-step guide to adding your own decoding model</li> <li>Adding a New Task - How to implement custom decoding tasks</li> <li>Configuration Guide - Understanding and configuring experiments</li> <li>Task Reference - Complete reference for all available tasks</li> <li>Baseline Results - Performance benchmarks for all tasks</li> <li>Registry API Reference - Registry decorators and function signatures</li> </ol>"},{"location":"#overview","title":"Overview","text":"<p>This framework provides a flexible system for: - Training neural decoding models on iEEG data - Comparing different model architectures - Evaluating performance across multiple metrics - Running systematic hyperparameter searches</p> <p>For long updates and discussions, see this notebook.</p>"},{"location":"adding-task/","title":"Adding a New Task","text":"<p>Guide to implementing custom decoding tasks beyond word embedding prediction.</p>"},{"location":"adding-task/#overview","title":"Overview","text":"<p>A task defines what you're trying to decode from neural data. The default task is word embedding decoding, but you can create tasks for any prediction target aligned with temporal events in your data: - Phoneme prediction - Sentiment classification - Grammatical role prediction - Part-of-speech tagging - Syllable-level features - Any other prediction target with associated timing information</p>"},{"location":"adding-task/#quick-reference","title":"Quick Reference","text":"<p>To add a new task:</p> <ol> <li>Define a task config dataclass</li> <li>Create a task data getter function</li> <li>Register the function</li> <li>Update your config</li> <li>Optional: Using input_fields</li> <li>Optional: Add custom metrics</li> </ol>"},{"location":"adding-task/#1-define-task-config-dataclass","title":"1. Define Task Config Dataclass","text":"<p>Create a dataclass in your task file that defines the task-specific configuration parameters.</p> <pre><code>from dataclasses import dataclass\nfrom core.config import BaseTaskConfig\n\n@dataclass\nclass MyTaskConfig(BaseTaskConfig):\n    \"\"\"Configuration for my_task.\"\"\"\n    csv_path: str = \"processed_data/my_task_data.csv\"\n    threshold: float = 0.5\n    use_special_mode: bool = False\n</code></pre> <p>Important: - Must inherit from <code>BaseTaskConfig</code> - Define all task-specific parameters with type hints - Provide sensible defaults where appropriate - Do NOT duplicate fields that belong in <code>DataParams</code> (like <code>data_root</code>, <code>window_width</code>, <code>subject_ids</code>) - <code>BaseTaskConfig</code> includes an <code>input_fields</code> parameter (optional list of column names from your DataFrame to pass as additional model inputs)</p>"},{"location":"adding-task/#2-create-task-data-getter","title":"2. Create Task Data Getter","text":"<p>Create a new file in <code>tasks/</code> with a function that loads and processes your task-specific data.</p>"},{"location":"adding-task/#function-signature","title":"Function Signature","text":"<pre><code>from core.config import TaskConfig\nimport pandas as pd\n\ndef my_task_data_getter(task_config: TaskConfig) -&gt; pd.DataFrame:\n    \"\"\"\n    Load task-specific data.\n\n    Args:\n        task_config: TaskConfig containing data_params and task_specific_config\n\n    Returns:\n        DataFrame with required columns:\n        - start: Time to center neural data around (seconds)\n        - target: Target variable for prediction\n        - word: (Optional) The text/label for this event\n    \"\"\"\n    # Access task-specific config\n    config: MyTaskConfig = task_config.task_specific_config\n    # Access shared data params\n    data_params = task_config.data_params\n\n    # Your implementation here\n    pass\n</code></pre> <p>Required DataFrame Columns: - <code>start</code> (float): Timestamp to center the neural data window around (in seconds) - <code>target</code> (any): The prediction target (can be embeddings, labels, scalars, etc.)</p> <p>Optional Columns: - <code>word</code> (str): Text/label for the event (useful for zero-shot folds) - Any columns specified in <code>input_fields</code> (will be passed as kwargs to the model) - Any other metadata you want to track</p>"},{"location":"adding-task/#minimal-example","title":"Minimal Example","text":"<pre><code>from dataclasses import dataclass\nfrom core.config import BaseTaskConfig, TaskConfig\nimport pandas as pd\nimport os\nimport core.registry as registry\n\n@dataclass\nclass ConstantPredictionConfig(BaseTaskConfig):\n    \"\"\"Configuration for constant_prediction_task.\"\"\"\n    target_value: float = 1.0\n\n@registry.register_task_data_getter(config_type=ConstantPredictionConfig)\ndef constant_prediction_task(task_config: TaskConfig):\n    \"\"\"Simple task: predict a constant value.\"\"\"\n    config: ConstantPredictionConfig = task_config.task_specific_config\n    data_params = task_config.data_params\n\n    # Load timing data\n    transcript_path = os.path.join(\n        data_params.data_root,\n        \"stimuli/gpt2-xl/transcript.tsv\"\n    )\n    df = pd.read_csv(transcript_path, sep=\"\\t\")\n\n    # Group tokens into words and get start times\n    df_word = df.groupby(\"word_idx\").agg(dict(start=\"first\"))\n\n    # Set target to constant (model learns to output the configured value)\n    df_word[\"target\"] = config.target_value\n\n    return df_word\n</code></pre>"},{"location":"adding-task/#3-register-the-function","title":"3. Register the Function","text":"<p>Use the <code>@registry.register_task_data_getter()</code> decorator with the <code>config_type</code> parameter:</p> <pre><code>import core.registry as registry\n\n@registry.register_task_data_getter(config_type=MyTaskConfig)\ndef my_custom_task(task_config: TaskConfig):\n    config: MyTaskConfig = task_config.task_specific_config\n    data_params = task_config.data_params\n    # Your implementation\n    return df_word\n</code></pre> <p>Important: - The <code>config_type</code> parameter is required and must be your task config dataclass - The function name will be used as the task name in configs (unless you override with <code>name</code> parameter)</p> <p>Optional: Specify a custom name: <pre><code>@registry.register_task_data_getter(name='custom_name', config_type=MyTaskConfig)\ndef my_function(task_config: TaskConfig):\n    ...\n</code></pre></p>"},{"location":"adding-task/#4-update-config","title":"4. Update Config","text":"<p>Create a YAML config with the new nested structure:</p> <pre><code>task_config:\n  task_name: my_custom_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n    # ... other shared data params\n  task_specific_config:\n    csv_path: \"processed_data/my_task_data.csv\"\n    threshold: 0.5\n    use_special_mode: true\n\nmodel_constructor_name: my_model\nmodel_params:\n  # ... model params\n\ntraining_params:\n  # ... training params\n</code></pre> <p>The task-specific parameters are now type-safe and validated at runtime!</p>"},{"location":"adding-task/#5-optional-using-input_fields","title":"5. Optional: Using input_fields","text":"<p>If your model needs additional inputs beyond neural data, use the <code>input_fields</code> parameter to specify which DataFrame columns should be passed to your model as kwargs.</p>"},{"location":"adding-task/#example-passing-word-ids-to-a-model","title":"Example: Passing word IDs to a model","text":"<pre><code>@dataclass\nclass MyTaskConfig(BaseTaskConfig):\n    \"\"\"Configuration for task requiring word IDs.\"\"\"\n    input_fields: Optional[list[str]] = field(default_factory=lambda: [\"word_id\"])\n\n@registry.register_task_data_getter(config_type=MyTaskConfig)\ndef my_task(task_config: TaskConfig):\n    config: MyTaskConfig = task_config.task_specific_config\n\n    # Create DataFrame with required columns\n    df = pd.DataFrame({\n        'start': [0.0, 1.0, 2.0],\n        'target': [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]],\n        'word_id': [42, 43, 44]  # This will be passed to model\n    })\n    return df\n</code></pre> <p>Your model's forward method should accept these fields as keyword arguments:</p> <pre><code>def forward(self, neural_data, word_id=None, **kwargs):\n    # word_id will be a tensor of shape [batch_size]\n    if word_id is not None:\n        # Use word_id in your model\n        embeddings = self.word_embedding(word_id)\n    # ...\n</code></pre> <p>Important: - All fields in <code>input_fields</code> must be columns in the returned DataFrame - These columns will be converted to tensors and passed as kwargs during training - Handle None values in your model if the field might not be provided</p>"},{"location":"adding-task/#6-optional-custom-metrics","title":"6. Optional: Custom Metrics","text":"<p>Define metrics specific to your task. Add them to the appropriate file in the <code>metrics/</code> package based on the metric type:</p> <ul> <li>Regression metrics \u2192 <code>metrics/regression_metrics.py</code></li> <li>Classification metrics \u2192 <code>metrics/classification_metrics.py</code></li> <li>Embedding metrics \u2192 <code>metrics/embedding_metrics.py</code></li> <li>Utility functions \u2192 <code>metrics/utils.py</code></li> </ul> <p>Example:</p> <pre><code>import torch\nfrom core.registry import register_metric\n\n@register_metric('my_accuracy')\ndef my_accuracy_metric(predicted: torch.Tensor, groundtruth: torch.Tensor):\n    \"\"\"\n    Custom metric for your task.\n\n    Args:\n        predicted: Model predictions [batch_size, ...]\n        groundtruth: Ground truth targets [batch_size, ...]\n\n    Returns:\n        Scalar metric value\n    \"\"\"\n    correct = (predicted.argmax(dim=1) == groundtruth).float()\n    return correct.mean()\n</code></pre> <p>Then add to your config: <pre><code>training_params:\n  losses: [cross_entropy]\n  metrics: [my_accuracy, cosine_sim]\n</code></pre></p> <p>The metrics are automatically registered when the package is imported.</p>"},{"location":"adding-task/#examples","title":"Examples","text":"<p>See the <code>tasks/</code> directory for complete examples: - <code>tasks/word_embedding.py</code> - Word embedding decoding (WordEmbeddingConfig) - <code>tasks/sentence_onset.py</code> - Binary classification (SentenceOnsetConfig) - <code>tasks/content_noncontent.py</code> - Binary classification (ContentNonContentConfig) - <code>tasks/pos_task.py</code> - Multi-class classification (PosTaskConfig) - <code>tasks/gpt_surprise.py</code> - Regression task (GptSurpriseConfig) - <code>tasks/volume_level.py</code> - Audio feature prediction (VolumeLevelConfig)</p> <p>For detailed documentation on all available tasks, see the Task Reference. For baseline performance benchmarks, see Baseline Results.</p>"},{"location":"adding-task/#see-also","title":"See Also","text":"<ul> <li>Task Reference - Complete reference for all available tasks</li> <li>Baseline Results - Performance benchmarks for all tasks</li> <li>Configuration Guide - How to configure tasks</li> <li>API Reference - Task data getter API</li> <li><code>tasks/</code> directory - Complete task examples</li> </ul>"},{"location":"api-reference/","title":"Registry API Reference","text":"<p>Reference for all registry decorators and their function signatures.</p>"},{"location":"api-reference/#overview","title":"Overview","text":"<p>The framework uses registries to discover and manage model components. Decorate your functions with the appropriate registry decorator to make them available to the training pipeline.</p> <p>Module: <code>core/registry.py</code></p>"},{"location":"api-reference/#register_model_constructornamenone","title":"<code>@register_model_constructor(name=None)</code>","text":"<p>Register a function that constructs your decoding model.</p>"},{"location":"api-reference/#purpose","title":"Purpose","text":"<p>Creates model instances from config parameters. Called during training setup.</p>"},{"location":"api-reference/#function-signature","title":"Function Signature","text":"<pre><code>def model_constructor(model_params: dict) -&gt; nn.Module\n</code></pre>"},{"location":"api-reference/#arguments","title":"Arguments","text":"<ul> <li><code>model_params</code> (dict): Parameters from your config's <code>model_params</code> section</li> </ul>"},{"location":"api-reference/#returns","title":"Returns","text":"<ul> <li>PyTorch model instance</li> </ul>"},{"location":"api-reference/#example","title":"Example","text":"<pre><code>@registry.register_model_constructor()\ndef my_model(model_params):\n    return MyModel(\n        input_dim=model_params['input_dim'],\n        output_dim=model_params['output_dim']\n    )\n</code></pre>"},{"location":"api-reference/#usage-in-config","title":"Usage in Config","text":"<pre><code>model_constructor_name: my_model\nmodel_params:\n  input_dim: 256\n  output_dim: 50\n</code></pre>"},{"location":"api-reference/#register_data_preprocessornamenone","title":"<code>@register_data_preprocessor(name=None)</code>","text":"<p>Register a function that preprocesses neural data.</p>"},{"location":"api-reference/#purpose_1","title":"Purpose","text":"<p>Transforms raw neural data into the format your model expects. Called once before training.</p>"},{"location":"api-reference/#function-signature_1","title":"Function Signature","text":"<pre><code>def preprocessor(\n    data: np.ndarray,  # [num_events, num_electrodes, timesteps]\n    preprocessor_params: dict\n) -&gt; np.ndarray  # [num_events, ...]\n</code></pre>"},{"location":"api-reference/#arguments_1","title":"Arguments","text":"<ul> <li><code>data</code> (np.ndarray): Raw neural data with shape <code>[num_events, num_electrodes, timesteps]</code></li> <li><code>preprocessor_params</code> (dict): Parameters from your config's <code>data_params.preprocessor_params</code></li> </ul>"},{"location":"api-reference/#returns_1","title":"Returns","text":"<ul> <li>Preprocessed data with shape <code>[num_events, ...]</code> (any shape your model needs)</li> </ul>"},{"location":"api-reference/#example_1","title":"Example","text":"<pre><code>@registry.register_data_preprocessor()\ndef my_preprocessor(data, preprocessor_params):\n    # Average over time\n    n_avg = preprocessor_params['num_average_samples']\n    return data.reshape(data.shape[0], data.shape[1], -1, n_avg).mean(-1)\n</code></pre>"},{"location":"api-reference/#usage-in-config_1","title":"Usage in Config","text":"<pre><code>data_params:\n  preprocessing_fn_name: my_preprocessor\n  preprocessor_params:\n    num_average_samples: 32\n</code></pre>"},{"location":"api-reference/#register_config_setternamenone","title":"<code>@register_config_setter(name=None)</code>","text":"<p>Register a function that modifies config at runtime based on loaded data.</p>"},{"location":"api-reference/#purpose_2","title":"Purpose","text":"<p>Sets config values that depend on the data (e.g., number of channels, model dimensions). Called after data is loaded, before model construction.</p>"},{"location":"api-reference/#function-signature_2","title":"Function Signature","text":"<pre><code>def config_setter(\n    experiment_config: ExperimentConfig,\n    raws: list[mne.io.Raw],\n    df_word: pd.DataFrame\n) -&gt; ExperimentConfig\n</code></pre>"},{"location":"api-reference/#arguments_2","title":"Arguments","text":"<ul> <li><code>experiment_config</code> (ExperimentConfig): Your experiment configuration</li> <li><code>raws</code> (list[mne.io.Raw]): Loaded neural recordings</li> <li><code>df_word</code> (pd.DataFrame): Task data with event timings and targets</li> </ul>"},{"location":"api-reference/#returns_2","title":"Returns","text":"<ul> <li>Modified <code>ExperimentConfig</code></li> </ul>"},{"location":"api-reference/#example_2","title":"Example","text":"<pre><code>@registry.register_config_setter()\ndef my_config_setter(experiment_config, raws, df_word):\n    # Set input channels based on loaded data\n    num_channels = sum([len(raw.ch_names) for raw in raws])\n    experiment_config.model_params['input_channels'] = num_channels\n    return experiment_config\n</code></pre>"},{"location":"api-reference/#usage-in-config_2","title":"Usage in Config","text":"<pre><code>config_setter_name: my_config_setter\n</code></pre>"},{"location":"api-reference/#register_metricnamenone","title":"<code>@register_metric(name=None)</code>","text":"<p>Register a metric or loss function.</p>"},{"location":"api-reference/#purpose_3","title":"Purpose","text":"<p>Defines objectives for training (losses) or evaluation (metrics). Called during each training step.</p>"},{"location":"api-reference/#function-signature_3","title":"Function Signature","text":"<pre><code>def metric(\n    predicted: torch.Tensor,\n    groundtruth: torch.Tensor\n) -&gt; float\n</code></pre>"},{"location":"api-reference/#arguments_3","title":"Arguments","text":"<ul> <li><code>predicted</code> (torch.Tensor): Model predictions <code>[batch_size, ...]</code></li> <li><code>groundtruth</code> (torch.Tensor): Ground truth targets <code>[batch_size, ...]</code></li> </ul>"},{"location":"api-reference/#returns_3","title":"Returns","text":"<ul> <li>Scalar metric value (float or torch scalar)</li> </ul>"},{"location":"api-reference/#example_3","title":"Example","text":"<pre><code>@registry.register_metric()\ndef my_loss(predicted, groundtruth):\n    return F.mse_loss(predicted, groundtruth)\n</code></pre>"},{"location":"api-reference/#usage-in-config_3","title":"Usage in Config","text":"<pre><code>training_params:\n  losses: [my_loss, mse]\n  loss_weights: [0.5, 0.5]\n  metrics: [cosine_sim]\n</code></pre>"},{"location":"api-reference/#register_task_data_getternamenone","title":"<code>@register_task_data_getter(name=None)</code>","text":"<p>Register a function that loads task-specific data.</p>"},{"location":"api-reference/#purpose_4","title":"Purpose","text":"<p>Loads event timings and targets for your decoding task. Called once at the start of training.</p>"},{"location":"api-reference/#function-signature_4","title":"Function Signature","text":"<pre><code>def task_data_getter(data_params: DataParams) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/#arguments_4","title":"Arguments","text":"<ul> <li><code>data_params</code> (DataParams): Data configuration from your config file</li> </ul>"},{"location":"api-reference/#returns_4","title":"Returns","text":"<ul> <li>DataFrame with required columns:</li> <li><code>start</code> (float): Event onset time in seconds</li> <li><code>target</code> (any): Prediction target (embeddings, labels, etc.)</li> <li><code>word</code> (str, optional): Event label (for zero-shot folds)</li> </ul>"},{"location":"api-reference/#example_4","title":"Example","text":"<pre><code>@registry.register_task_data_getter()\ndef my_task(data_params):\n    # Load timing data\n    df = pd.read_csv(data_params.task_params['data_file'])\n\n    # Create required columns\n    df['start'] = df['onset_time']\n    df['target'] = df['label'].values\n\n    return df[['start', 'target']]\n</code></pre>"},{"location":"api-reference/#usage-in-config_4","title":"Usage in Config","text":"<pre><code>task_name: my_task\ndata_params:\n  task_params:\n    data_file: path/to/data.csv\n</code></pre>"},{"location":"api-reference/#built-in-registered-functions","title":"Built-in Registered Functions","text":""},{"location":"api-reference/#models","title":"Models","text":"<p>See <code>models/neural_conv_decoder/decoder_model.py</code> and <code>models/example_foundation_model/integration.py</code> for examples.</p>"},{"location":"api-reference/#preprocessors","title":"Preprocessors","text":"<ul> <li><code>window_average_neural_data</code> - Temporal averaging (models/neural_conv_decoder)</li> <li><code>foundation_model_preprocessing_fn</code> - Extract frozen foundation model features</li> <li><code>foundation_model_finetune_mlp</code> - Prepare data for foundation model finetuning</li> </ul>"},{"location":"api-reference/#metrics","title":"Metrics","text":"<p>The metrics package is organized by task type:</p> <p>Regression Metrics (<code>metrics/regression_metrics.py</code>): - <code>mse</code> - Mean squared error - <code>corr</code> - Pearson correlation coefficient - <code>r2</code> - R\u00b2 score (coefficient of determination)</p> <p>Embedding Metrics (<code>metrics/embedding_metrics.py</code>): - <code>cosine_sim</code> - Cosine similarity - <code>cosine_dist</code> - Cosine distance - <code>nll_embedding</code> - Contrastive NLL - <code>similarity_entropy</code> - Similarity distribution entropy</p> <p>Classification Metrics (<code>metrics/classification_metrics.py</code>): - <code>bce</code> - Binary cross-entropy (weighted) - <code>cross_entropy</code> - Multi-class cross-entropy - <code>roc_auc</code> - ROC-AUC for binary classification - <code>roc_auc_multiclass</code> - ROC-AUC for multi-class classification - <code>f1</code> - F1 score - <code>sensitivity</code> - Sensitivity (recall/TPR) - <code>precision</code> - Precision - <code>specificity</code> - Specificity (TNR) - <code>confusion_matrix</code> - Confusion matrix - <code>perplexity</code> - Perplexity (for LLM evaluation)</p> <p>Utility Functions (<code>metrics/utils.py</code>): - <code>compute_cosine_distances</code> - Cosine distance computation with ensemble support - <code>compute_class_scores</code> - Convert distances to class probabilities - <code>calculate_auc_roc</code> - AUC-ROC with frequency filtering - <code>top_k_accuracy</code> - Top-k accuracy calculation - <code>entropy</code> - Entropy computation for distributions</p> <p>See the <code>metrics/</code> package for complete implementations.</p>"},{"location":"api-reference/#tasks","title":"Tasks","text":"<ul> <li><code>word_embedding_decoding_task</code> - Decode word embeddings (default)</li> <li><code>placeholder_task</code> - Minimal example</li> <li><code>content_noncontent_task</code> - Content vs non-content classification</li> <li><code>gpt_surprise_task</code> - GPT surprisal prediction</li> <li><code>gpt_surprise_multiclass_task</code> - GPT surprisal multiclass classification</li> <li><code>pos_task</code> - Part-of-speech tagging</li> <li><code>sentence_onset_task</code> - Sentence onset detection</li> <li><code>volume_level_encoding_task</code> - Audio volume level prediction</li> </ul> <p>See <code>tasks/</code> directory for implementations.</p>"},{"location":"api-reference/#see-also","title":"See Also","text":"<ul> <li>Onboarding a Model - How to use registries</li> <li>Adding a Task - Task data getter details</li> <li>Configuration Guide - Config structure</li> </ul>"},{"location":"baseline-results/","title":"Baseline Results Summary","text":"<p>This page summarizes the baseline results for all tasks in the podcast benchmark.</p>"},{"location":"baseline-results/#overview","title":"Overview","text":"<p>Baseline results for all of our tasks using a simple deep network, trained only on our data.</p> <p>Note: For detailed metrics across all lags, see the <code>lag_performance.csv</code> file in each task's results directory linked below.</p>"},{"location":"baseline-results/#contentnon-content-classification","title":"Content/Non-Content Classification","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_content_noncontent.yml</code></p> <p>Detailed Results: <code>baseline-results/content_noncontent_task_sig_elecs_mlp_early_stop_roc_2025-12-19-00-34-17/lag_performance.csv</code></p> <p></p> <p>Best Performance:</p> <ul> <li>Lag: 200ms</li> <li>ROC-AUC: 0.5900</li> </ul>"},{"location":"baseline-results/#word-embedding-decoding","title":"Word Embedding Decoding","text":""},{"location":"baseline-results/#performance-across-lags","title":"Performance Across Lags","text":""},{"location":"baseline-results/#best-performance-by-model","title":"Best Performance by Model","text":""},{"location":"baseline-results/#arbitrary","title":"Arbitrary","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_arbitrary.yml</code></p> <p>Detailed Results: <code>baseline-results/ensemble_model_10_arbitrary_2025-12-19-00-17-32/lag_performance.csv</code></p> <p>Best Performance:</p> <ul> <li>Lag: 400ms</li> <li>AUC-ROC: 0.5549</li> </ul>"},{"location":"baseline-results/#glove","title":"GloVe","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_glove.yml</code></p> <p>Detailed Results: <code>baseline-results/ensemble_model_10_glove_2025-12-19-00-17-41/lag_performance.csv</code></p> <p>Best Performance:</p> <ul> <li>Lag: 400ms</li> <li>AUC-ROC: 0.6046</li> </ul>"},{"location":"baseline-results/#gpt-2","title":"GPT-2","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_gpt2.yml</code></p> <p>Detailed Results: <code>baseline-results/ensemble_model_10_gpt2_2025-12-19-00-17-43/lag_performance.csv</code></p> <p>Best Performance:</p> <ul> <li>Lag: 400ms</li> <li>AUC-ROC: 0.6057</li> </ul>"},{"location":"baseline-results/#gpt-surprisal-regression","title":"GPT Surprisal (Regression)","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_gpt_surprise.yml</code></p> <p>Detailed Results: <code>baseline-results/gpt_surprise_2025-12-19-00-18-44/lag_performance.csv</code></p> <p></p> <p>Best Performance:</p> <ul> <li>Lag: 400ms</li> <li>Correlation: 0.0591</li> </ul>"},{"location":"baseline-results/#gpt-surprisal-multiclass","title":"GPT Surprisal (Multiclass)","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_gpt_surprise_multiclass.yml</code></p> <p>Detailed Results: <code>baseline-results/gpt_surprise_2025-12-19-00-18-43/lag_performance.csv</code></p> <p></p> <p>Best Performance:</p> <ul> <li>Lag: 200ms</li> <li>ROC-AUC (Multiclass): 0.5333</li> </ul>"},{"location":"baseline-results/#part-of-speech","title":"Part of Speech","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_pos.yml</code></p> <p>Detailed Results: <code>baseline-results/pos_task_sig_elecs_without_other_classes_2025-12-19-00-34-17/lag_performance.csv</code></p> <p></p> <p>Best Performance:</p> <ul> <li>Lag: 600ms</li> <li>ROC-AUC (Multiclass): 0.5305</li> </ul>"},{"location":"baseline-results/#sentence-onset-detection","title":"Sentence Onset Detection","text":"<p>Config: <code>configs/neural_conv_decoder/neural_conv_decoder_sentence_onset.yml</code></p> <p>Detailed Results: <code>baseline-results/sentence_onset_lr_2025-12-19-00-18-44/lag_performance.csv</code></p> <p></p> <p>Best Performance:</p> <ul> <li>Lag: 0ms</li> <li>ROC-AUC: 0.8800</li> </ul>"},{"location":"baseline-results/#volume-level-prediction","title":"Volume Level Prediction","text":"<p>Config: <code>configs/time_pooling_model/simple_model.yml</code></p> <p>Detailed Results: <code>baseline-results/volume_level_simple_2025-12-19-00-34-56/lag_performance.csv</code></p> <p></p> <p>Best Performance:</p> <ul> <li>Lag: 200ms</li> <li>Correlation: 0.4479</li> </ul>"},{"location":"baseline-results/#llm-token-decoding","title":"LLM Token Decoding","text":"<p>This section compares two approaches to LLM-based decoding from brain activity: one using brain data (LLM Token Finetuning) and a control without brain data (LLM Decoding).</p> <p></p>"},{"location":"baseline-results/#llm-token-finetuning-brain-data","title":"LLM Token Finetuning (Brain Data)","text":"<p>Config: <code>configs/neural_conv_decoder/llm_two_stage_multi.yml</code></p> <p>Detailed Results: <code>baseline-results/llm_token_finetune_2025-12-26-12-44-36/lag_performance.csv</code></p> <p>Best Performance:</p> <ul> <li>Lag: 200ms</li> <li>Perplexity: 60.40</li> </ul>"},{"location":"baseline-results/#llm-decoding-no-brain-data-control","title":"LLM Decoding (No Brain Data - Control)","text":"<p>Config: <code>configs/controls/llm_decoding_no_brain_data.yml</code></p> <p>Detailed Results: <code>baseline-results/llm_decoding_control_2025-12-28-15-55-38/lag_performance.csv</code></p> <p>Best Performance:</p> <ul> <li>Lag: -200ms</li> <li>Perplexity: 67.22</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>Complete guide to configuring experiments in the podcast benchmark framework.</p>"},{"location":"configuration/#overview","title":"Overview","text":"<p>All experiments are configured via YAML files in the <code>configs/</code> directory. The framework supports two configuration types:</p> <ol> <li>Single Task Configuration - Train one model on one task</li> <li>Multi-Task Configuration - Train multiple tasks sequentially (e.g., pretraining + finetuning)</li> </ol>"},{"location":"configuration/#single-task-configuration-structure","title":"Single Task Configuration Structure","text":"<pre><code># Model specification (supports nested sub-models)\nmodel_spec:\n  constructor_name: my_model\n  params:\n    # Model-specific parameters (passed to constructor)\n  sub_models:\n    # Optional: nested models passed as constructor arguments\n    # encoder_model: {...}\n  checkpoint_path: null  # Optional: path to checkpoint for initialization\n\n# Optional: single setter or list of setters to apply\nconfig_setter_name: my_config_setter  # or [setter1, setter2]\n\n# Task configuration (nested structure)\ntask_config:\n  task_name: word_embedding_decoding_task\n  data_params:\n    # Shared data parameters (subjects, electrodes, window size, etc.)\n  task_specific_config:\n    # Task-specific parameters (type-safe, defined per task)\n\n# How to train\ntraining_params:\n  # Batch size, learning rate, losses, metrics, etc.\n\n# Where to save results\noutput_dir: results\ncheckpoint_dir: checkpoints\ntensorboard_dir: event_logs\ntrial_name: my_experiment\n</code></pre>"},{"location":"configuration/#multi-task-configuration-structure","title":"Multi-Task Configuration Structure","text":"<p>For multi-stage training (e.g., pretraining then finetuning), use the multi-task configuration format:</p> <pre><code># List of tasks to run sequentially\ntasks:\n  - # First task (e.g., pretraining)\n    trial_name: pretrain\n    model_spec:\n      constructor_name: my_encoder\n      params:\n        input_channels: 64\n        output_dim: 768\n    task_config:\n      task_name: word_embedding_decoding_task\n      data_params:\n        subject_ids: [1, 2, 3]\n    training_params:\n      epochs: 100\n      n_folds: 5\n\n  - # Second task (e.g., finetuning)\n    trial_name: finetune\n    model_spec:\n      constructor_name: my_decoder\n      params:\n        freeze_encoder: true\n      sub_models:\n        encoder:\n          constructor_name: my_encoder\n          params:\n            input_channels: 64\n            output_dim: 768\n          # Reference previous task's checkpoints\n          checkpoint_path: \"{prev_checkpoint_dir}/lag_{lag}/best_model_fold{fold}.pt\"\n    task_config:\n      task_name: sentence_classification_task\n      data_params:\n        subject_ids: [1, 2, 3]\n    training_params:\n      epochs: 50\n      n_folds: 5\n\n# Optional: override parameters across all tasks\nshared_params:\n  training_params.n_folds: 5\n  training_params.random_seed: 42\n  task_config.data_params.subject_ids: [1, 2, 3]\n</code></pre> <p>Key Features: - Sequential Execution: Tasks run in order, with access to previous results - <code>{prev_checkpoint_dir}</code>: Reference checkpoints from the previous task - <code>shared_params</code>: Override the same parameter across all tasks (applied after config setters) - Per-Task Configuration: Each task has its own complete <code>ExperimentConfig</code></p>"},{"location":"configuration/#model-specification","title":"Model Specification","text":"<p>Purpose: Define your model architecture with support for nested sub-models and checkpoint initialization.</p> <p>The <code>model_spec</code> section specifies how to build your model: - constructor_name: Registered model constructor function name - params: Parameters passed to the constructor (fully customizable) - sub_models: Dictionary of nested models to build and pass as constructor arguments - checkpoint_path: Optional path to checkpoint for model initialization (supports placeholders)</p> <p>Simple Example: <pre><code>model_spec:\n  constructor_name: pitom_model\n  params:\n    input_channels: 64\n    output_dim: 768\n    conv_filters: 128\n    dropout: 0.2\n  sub_models: {}\n  checkpoint_path: null  # or path to checkpoint\n</code></pre></p> <p>Nested Model Example (e.g., GPT2Brain with encoder): <pre><code>model_spec:\n  constructor_name: gpt2_brain\n  params:\n    freeze_lm: true\n    device: cuda\n  sub_models:\n    encoder_model:\n      constructor_name: pitom_model\n      params:\n        input_channels: 64\n        output_dim: 768\n        conv_filters: 128\n        dropout: 0.2\n      sub_models: {}\n      checkpoint_path: \"checkpoints/encoder/lag_{lag}/best_model_fold{fold}.pt\"\n</code></pre></p> <p>In this example: 1. The <code>pitom_model</code> encoder is built first with the specified params 2. The encoder is initialized from a checkpoint (dynamically formatted with lag/fold) 3. The built encoder is then passed to <code>gpt2_brain</code> as the <code>encoder_model</code> argument 4. This allows you to train different encoders at each lag while using the same parent model</p> <p>Checkpoint Path Placeholders: - <code>{lag}</code>: Replaced with the current lag value during training - <code>{fold}</code>: Replaced with the current fold number during training - <code>{prev_checkpoint_dir}</code>: In multi-task configs, replaced with previous task's checkpoint directory</p> <p>Example checkpoint path with placeholders: <pre><code>checkpoint_path: \"checkpoints/pretrain/lag_{lag}/best_model_fold{fold}.pt\"\n# Becomes: checkpoints/pretrain/lag_0/best_model_fold0.pt (for lag=0, fold=0)\n</code></pre></p>"},{"location":"configuration/#training-parameters","title":"Training Parameters","text":"<p>Purpose: Control the training loop, optimization, and evaluation.</p> <p>Key concepts: - Losses and metrics: What to optimize and track - Cross-validation: How to split data into folds - Early stopping: When to stop training - Time lags: Temporal relationship between neural data and events</p> <p>Particularly useful fields:</p> <pre><code>training_params:\n  # Loss configuration\n  losses: [mse, cosine_dist]    # Can combine multiple losses\n  loss_weights: [0.7, 0.3]      # Weight for each loss\n  metrics: [cosine_sim]         # Additional metrics to track (not in loss)\n\n  # Early stopping\n  early_stopping_metric: cosine_sim  # What metric to monitor\n  smaller_is_better: false          # false for accuracy/similarity, true for error\n\n  # Cross-validation strategy\n  fold_type: sequential_folds   # or \"zero_shot_folds\" for words not in training\n  n_folds: 5\n\n  # Time lags - find optimal temporal offset\n  min_lag: -500      # Start 500ms before word onset\n  max_lag: 1000      # End 1000ms after word onset\n  lag_step_size: 100 # Test every 100ms\n\n  # Baseline models\n  linear_regression_baseline: false    # Train and evaluate linear regression baseline\n  logistic_regression_baseline: false  # Train and evaluate logistic regression baseline\n</code></pre> <p>See <code>core/config.py:TrainingParams</code> for all available fields.</p>"},{"location":"configuration/#task-configuration","title":"Task Configuration","text":"<p>Purpose: Specify the task, data parameters, and task-specific settings.</p> <p>The <code>task_config</code> section is a nested structure (defined by the <code>TaskConfig</code> dataclass in <code>core/config.py</code>) with three parts:</p> <ol> <li>task_name (<code>str</code>): Which task to run (e.g., <code>word_embedding_decoding_task</code>)</li> <li>data_params (<code>DataParams</code>): Shared data parameters used across all tasks</li> <li>task_specific_config (<code>BaseTaskConfig</code>): Task-specific parameters (type-safe, validated per task)</li> </ol> <p>This nested structure ensures type safety and clear separation between: - General data collection parameters (electrodes, preprocessing, subjects) - Task-specific logic (embeddings, targets, task-specific data paths)</p> <p>Example structure:</p> <pre><code>task_config:\n  task_name: sentence_onset_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n    preprocessing_fn_name: window_average_neural_data\n    preprocessor_params:\n      num_average_samples: 4\n  task_specific_config:\n    sentence_csv_path: processed_data/sentences.csv\n    negatives_per_positive: 5\n    negative_margin_s: 0.75\n</code></pre>"},{"location":"configuration/#data-parameters","title":"Data Parameters","text":"<p>Shared fields used across all tasks:</p>"},{"location":"configuration/#electrode-selection","title":"Electrode Selection","text":"<pre><code>task_config:\n  data_params:\n    # Option 1: Regular expression\n    channel_reg_ex: \"LG[AB]*\"\n\n    # Option 2: CSV file\n    electrode_file_path: configs/significant_electrodes.csv\n\n    # Option 3: Per-subject dictionary\n    per_subject_electrodes:\n      1: [LGA1, LGA2, LGA3]\n      2: [LGB1, LGB2]\n</code></pre>"},{"location":"configuration/#other-key-fields","title":"Other Key Fields","text":"<pre><code>task_config:\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625           # Width of neural data window (seconds)\n    word_column: lemmatized_word  # For zero-shot folds\n\n    # Preprocessing (single or list of preprocessors)\n    preprocessing_fn_name: my_preprocessor  # or [preprocessor1, preprocessor2]\n    preprocessor_params:  # Single dict or list of dicts\n      param1: value1\n</code></pre> <p>Note: <code>preprocessing_fn_name</code> and <code>preprocessor_params</code> can be either: - Single values: Apply one preprocessor - Lists: Apply multiple preprocessors in sequence (useful for chaining transformations)</p>"},{"location":"configuration/#task-specific-config","title":"Task-Specific Config","text":"<p>Each task defines its own config dataclass with type-safe parameters. See Task Reference for details on each task's configuration options.</p> <p>Example for word_embedding_decoding_task: <pre><code>task_config:\n  task_name: word_embedding_decoding_task\n  task_specific_config:\n    embedding_type: gpt-2xl\n    embedding_layer: 24\n    embedding_pca_dim: 50\n</code></pre></p> <p>Example for sentence_onset_task: <pre><code>task_config:\n  task_name: sentence_onset_task\n  task_specific_config:\n    sentence_csv_path: processed_data/sentences.csv\n    negatives_per_positive: 5\n    negative_margin_s: 0.75\n</code></pre></p> <p>Example with input_fields (pass additional DataFrame columns to model): <pre><code>task_config:\n  task_name: my_custom_task\n  task_specific_config:\n    input_fields: [word_id, position]  # Columns from DataFrame passed as model kwargs\n    # ... other task-specific params\n</code></pre></p>"},{"location":"configuration/#output-configuration","title":"Output Configuration","text":"<p>Purpose: Name your experiment and specify where results are saved.</p> <pre><code># Dynamic trial naming with formatting\ntrial_name: \"model_{}_lr={}_bs={}\"\nformat_fields:\n  - model_spec.params.model_name\n  - training_params.learning_rate\n  - training_params.batch_size\n\n# Output directories\noutput_dir: results              # CSV files with metrics\ncheckpoint_dir: checkpoints      # Saved model checkpoints\ntensorboard_dir: event_logs      # TensorBoard logs\n</code></pre> <p>Note: The <code>format_fields</code> parameter allows you to dynamically insert config values into the trial name using dot notation to access nested fields (e.g., <code>model_spec.params.dim</code> or <code>training_params.learning_rate</code>).</p>"},{"location":"configuration/#common-patterns","title":"Common Patterns","text":""},{"location":"configuration/#pattern-1-selecting-significant-electrodes-only","title":"Pattern 1: Selecting Significant Electrodes Only","text":"<pre><code>data_params:\n  electrode_file_path: configs/significant_electrodes.csv\n</code></pre>"},{"location":"configuration/#pattern-2-multi-loss-training","title":"Pattern 2: Multi-Loss Training","text":"<pre><code>training_params:\n  losses: [mse, cosine_dist]\n  loss_weights: [0.7, 0.3]\n  early_stopping_metric: cosine_sim  # Can use a metric not in losses\n</code></pre>"},{"location":"configuration/#pattern-3-finding-optimal-time-lag","title":"Pattern 3: Finding Optimal Time Lag","text":"<pre><code>training_params:\n  min_lag: -1000    # 1 second before word\n  max_lag: 2000     # 2 seconds after word\n  lag_step_size: 100\n\n# Results saved to {output_dir}/lag_performance.csv\n</code></pre>"},{"location":"configuration/#pattern-4-zero-shot-evaluation","title":"Pattern 4: Zero-Shot Evaluation","text":"<p>Test on words never seen during training:</p> <pre><code>training_params:\n  fold_type: zero_shot_folds\n\ndata_params:\n  word_column: lemmatized_word\n</code></pre>"},{"location":"configuration/#pattern-5-quick-debugging-run","title":"Pattern 5: Quick Debugging Run","text":"<pre><code>task_config:\n  data_params:\n    subject_ids: [1]  # Single subject\n\ntraining_params:\n  epochs: 10\n  n_folds: 2\n  lag: 0  # Single lag instead of sweep\n</code></pre>"},{"location":"configuration/#pattern-6-multi-task-training-pretraining-finetuning","title":"Pattern 6: Multi-Task Training (Pretraining + Finetuning)","text":"<pre><code>tasks:\n  - trial_name: pretrain_encoder\n    model_spec:\n      constructor_name: my_encoder\n      params:\n        input_channels: 64\n        output_dim: 768\n    task_config:\n      task_name: word_embedding_decoding_task\n      data_params:\n        subject_ids: [1, 2, 3]\n    training_params:\n      epochs: 100\n\n  - trial_name: finetune_decoder\n    model_spec:\n      constructor_name: my_decoder\n      sub_models:\n        encoder:\n          constructor_name: my_encoder\n          checkpoint_path: \"{prev_checkpoint_dir}/lag_{lag}/best_model_fold{fold}.pt\"\n    task_config:\n      task_name: sentence_classification_task\n      data_params:\n        subject_ids: [1, 2, 3]\n    training_params:\n      epochs: 50\n\nshared_params:\n  training_params.n_folds: 5\n  training_params.random_seed: 42\n</code></pre>"},{"location":"configuration/#pattern-7-multiple-config-setters","title":"Pattern 7: Multiple Config Setters","text":"<p>Apply multiple config setters in sequence:</p> <pre><code># Single setter\nconfig_setter_name: set_input_channels\n\n# Multiple setters (applied in order)\nconfig_setter_name: [set_input_channels, set_embedding_dim, initialize_model]\n</code></pre> <p>This is useful when you need to: - Set multiple dynamic parameters from data - Apply task-specific setters followed by model-specific setters - Chain transformations to the config</p>"},{"location":"configuration/#batch-training-with-training-matrix","title":"Batch Training with Training Matrix","text":"<p>The <code>training_matrix.yaml</code> file enables running multiple experiments at once. Define model/task/config combinations:</p> <pre><code>neural_conv_decoder:\n  word_embedding_decoding_task:\n    - neural_conv_decoder_base.yml\n    - neural_conv_decoder_binary.yml\n</code></pre> <p>Usage: <pre><code>make train-all                                    # Run all configs\nmake train-all MODELS=neural_conv_decoder         # Filter by model\nmake train-all TASKS=sentence_onset_task          # Filter by task\nmake train-all MODELS=model1,model2 TASKS=task1   # Combine filters\n</code></pre></p> <p>Adding New Model/Task Combinations:</p> <p>Edit <code>training_matrix.yaml</code> to add your experiments:</p> <pre><code>your_new_model:\n  your_new_task:\n    - config_file_1.yml\n    - config_file_2.yml\n    - config_file_3.yml\n</code></pre>"},{"location":"configuration/#see-also","title":"See Also","text":"<ul> <li><code>core/config.py</code>: Source code with all available fields and defaults</li> <li><code>training_matrix.yaml</code>: Batch experiment configuration</li> <li>Task Reference: Complete reference for all available tasks</li> <li>Onboarding a Model: How to use configs with your models</li> <li>API Reference: Detailed API documentation</li> </ul>"},{"location":"onboarding-model/","title":"Onboarding a New Model","text":"<p>Complete guide to adding your own decoding model to the framework.</p>"},{"location":"onboarding-model/#quick-reference","title":"Quick Reference","text":"<p>To set up a new model (e.g., BrainBERT), you need to:</p> <ol> <li>Create a new folder for your model code</li> <li>Define a decoding model and constructor function</li> <li>Define a data preprocessing function</li> <li>Create a config file</li> <li>Optional: Define a config setter function</li> <li>Import your module in main.py</li> <li>Optional: Update the Makefile</li> <li>Run your training code</li> </ol>"},{"location":"onboarding-model/#1-create-a-new-folder","title":"1. Create a New Folder","text":"<p>Organize all code for your model in its own directory inside the <code>models/</code> folder:</p> <pre><code>mkdir models/my_model\n</code></pre> <p>Write all model-specific code in this folder.</p>"},{"location":"onboarding-model/#2-define-decoding-model-and-constructor","title":"2. Define Decoding Model and Constructor","text":""},{"location":"onboarding-model/#define-your-model","title":"Define Your Model","text":"<p>Create your PyTorch model in <code>models/my_model/model.py</code>. For example:</p> <pre><code>import torch.nn as nn\n\nclass MyDecodingModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 512)\n        self.fc2 = nn.Linear(512, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n</code></pre>"},{"location":"onboarding-model/#create-a-constructor-function","title":"Create a Constructor Function","text":"<p>Define a constructor that takes a params dict (which may include sub-models) from your config:</p> <pre><code>import core.registry as registry\n\n@registry.register_model_constructor()\ndef my_model_constructor(params):\n    return MyDecodingModel(\n        input_dim=params['input_dim'],\n        output_dim=params['output_dim']\n    )\n</code></pre> <p>Important: - Use the <code>@registry.register_model_constructor()</code> decorator - The function must have signature: <code>constructor_fn(params: dict) -&gt; Model</code> - The <code>params</code> dict contains both regular parameters and any built sub-models - By default, the registered name is the function name (can override with <code>@registry.register_model_constructor('custom_name')</code>)</p>"},{"location":"onboarding-model/#examples","title":"Examples","text":"<p>Neural Conv Decoder (ensemble model): <pre><code>@registry.register_model_constructor()\ndef ensemble_pitom_model(params):\n    return EnsemblePitomModel(\n        num_models=params['num_models'],\n        input_channels=params['input_channels'],\n        output_dim=params['embedding_dim'],\n        conv_filters=params['conv_filters'],\n        dropout=params['dropout']\n    )\n</code></pre></p> <p>Model with Nested Sub-Model (e.g., GPT2Brain with encoder): <pre><code>@registry.register_model_constructor()\ndef gpt2_brain(params):\n    # params contains both regular params and built sub-models\n    return GPT2Brain(\n        lm_model=params['lm_model'],\n        tokenizer=params['tokenizer'],\n        encoder_model=params['encoder_model'],  # This is a pre-built model\n        device=params.get('device', 'cpu'),\n        freeze_lm=params.get('freeze_lm', True)\n    )\n</code></pre></p> <p>Foundation Model with Finetuning:</p> <p>When finetuning a foundation model, you include it as part of your decoder class:</p> <pre><code>class FoundationModelMLP(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        mlp_layer_sizes,\n        model_dir=None,\n        finetune=False,\n        foundation_model_config=None,\n        freeze_foundation_model=False,\n        num_unfrozen_blocks=0,\n    ):\n        super().__init__()\n        self.finetune = finetune\n\n        # Include foundation model as part of decoder if finetuning\n        if finetune:\n            self.foundation_model = create_and_freeze_foundation_model(\n                foundation_model_config,\n                model_dir,\n                freeze_foundation_model,\n                num_unfrozen_blocks,\n            )\n\n        self.embedding_norm = nn.BatchNorm1d(input_dim)\n        self.mlp = MLP(input_dim, mlp_layer_sizes)\n\n    def forward(self, x):\n        # Pass through foundation model if finetuning\n        if self.finetune:\n            x = self.foundation_model(x, forward_features=True)\n\n        x = self.embedding_norm(x)\n        return self.mlp(x)\n\n\n@registry.register_model_constructor()\ndef foundation_model_finetune_mlp(params):\n    return FoundationModelMLP(\n        params[\"model_dim\"],\n        params[\"mlp_layer_sizes\"],\n        model_dir=params.get(\"model_dir\"),\n        foundation_model_config=params[\"foundation_model_config\"],\n        finetune=True,\n        freeze_foundation_model=params.get(\"freeze_foundation_model\", False),\n        num_unfrozen_blocks=params.get(\"num_unfrozen_blocks\", 0),\n    )\n</code></pre> <p>Key Points for Finetuning: - Your decoder model includes the foundation model as a submodule - The foundation model is loaded with pretrained weights in <code>__init__</code> - You can optionally freeze parts of the foundation model - The <code>forward()</code> method runs data through both the foundation model and your decoder head</p>"},{"location":"onboarding-model/#3-define-data-preprocessing-function","title":"3. Define Data Preprocessing Function","text":"<p>Create a function to transform neural data for your model.</p> <pre><code>import core.registry as registry\n\n@registry.register_data_preprocessor()\ndef my_preprocessing_fn(data, preprocessor_params):\n    # data shape: [num_words, num_electrodes, timesteps]\n    # Return shape: [num_words, ...] (any shape your model expects)\n\n    # Example: average over time\n    return data.mean(axis=-1)\n</code></pre> <p>Function Signature: <pre><code>import numpy as np\n\ndef preprocessing_fn(\n    data: np.array,  # [num_words, num_electrodes, timesteps]\n    preprocessor_params: dict\n) -&gt; np.array  # [num_words, ...]\n</code></pre></p>"},{"location":"onboarding-model/#examples_1","title":"Examples","text":"<p>Neural Conv Decoder (temporal averaging): <pre><code>@registry.register_data_preprocessor()\ndef window_average_neural_data(data, preprocessor_params):\n    # Average over num_average_samples to reduce sample rate\n    return data.reshape(\n        data.shape[0],\n        data.shape[1],\n        -1,\n        preprocessor_params['num_average_samples']\n    ).mean(-1)\n</code></pre></p> <p>Foundation Model with Finetuning (prepare for model input):</p> <p>When finetuning, your preprocessing function prepares the data in the format your foundation model expects:</p> <pre><code>@registry.register_data_preprocessor(\"foundation_model_finetune_mlp\")\ndef prepare_data_for_finetuning(data, preprocessor_params):\n    \"\"\"Prepare neural data for foundation model input.\"\"\"\n    data_config = preprocessor_params[\"ecog_data_config\"]\n\n    # Downsample temporal resolution\n    data = data.reshape(\n        data.shape[0],\n        data.shape[1],\n        -1,\n        data_config.original_fs // data_config.new_fs\n    )\n    data = data.mean(-1)\n\n    # Pad to expected electrode grid (e.g., 64 channels)\n    for i in range(64):\n        channel = \"G\" + str(i + 1)\n        if channel not in preprocessor_params['ch_names']:\n            # Insert NaN for missing channels\n            data = np.insert(data, i, np.nan, axis=1)\n\n    # Reshape to spatial grid: [num_examples, bands, time, height, width]\n    data = np.einsum('bet-&gt;bte', data).reshape(data.shape[0], data.shape[2], 8, 8)\n    data = np.expand_dims(data, axis=1)\n\n    return data\n</code></pre> <p>Key Points: - When not finetuning: Extract frozen representations in preprocessing, return embeddings - When finetuning: Format raw data for model input, let the model extract features during training</p>"},{"location":"onboarding-model/#4-create-config-file","title":"4. Create Config File","text":"<p>Create a YAML config file in <code>configs/my_model/config.yml</code>.</p> <p>See Configuration Guide for detailed documentation on all config options.</p>"},{"location":"onboarding-model/#basic-example","title":"Basic Example","text":"<pre><code># Model specification\nmodel_spec:\n  constructor_name: my_model_constructor\n  params:\n    input_dim: 256\n    output_dim: 50\n  sub_models: {}\n\n# Optional: config setter function name\nconfig_setter_name: my_config_setter\n\n# Task configuration\ntask_config:\n  task_name: word_embedding_decoding_task\n  data_params:\n    data_root: data\n    window_width: 0.625\n    preprocessing_fn_name: my_preprocessing_fn\n    subject_ids: [1, 2, 3]\n    preprocessor_params:\n      custom_param: value\n  task_specific_config:\n    embedding_type: gpt-2xl\n    embedding_layer: 24\n\n# Training parameters\ntraining_params:\n  batch_size: 32\n  epochs: 100\n  learning_rate: 0.001\n  weight_decay: 0.0001\n  early_stopping_patience: 10\n  n_folds: 5\n  losses: [mse]\n  metrics: [cosine_sim]\n  early_stopping_metric: cosine_sim\n\n# Trial identifier\ntrial_name: my_model_v1\n</code></pre>"},{"location":"onboarding-model/#nested-model-example-eg-gpt2brain-with-encoder","title":"Nested Model Example (e.g., GPT2Brain with encoder)","text":"<pre><code>model_spec:\n  constructor_name: gpt2_brain\n  params:\n    freeze_lm: true\n    device: cuda\n  sub_models:\n    encoder_model:\n      constructor_name: pitom_model\n      params:\n        input_channels: 64\n        output_dim: 768\n        conv_filters: 128\n        dropout: 0.2\n      sub_models: {}\n      checkpoint_path: \"checkpoints/encoder/lag_{lag}/best_model_fold{fold}.pt\"\n\nconfig_setter_name: set_input_channels\n\n# This allows training different encoders at each lag\n# while reusing the same parent GPT2Brain model\n\ntask_config:\n  task_name: word_embedding_decoding_task\n  data_params:\n    data_root: data\n    preprocessing_fn_name: foundation_model_finetune_mlp\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    embedding_type: gpt-2xl\n    embedding_layer: 24\n    embedding_pca_dim: 50\n\ntraining_params:\n  batch_size: 64\n  learning_rate: 0.001\n  losses: [mse]\n  metrics: [cosine_sim, nll_embedding]\n  early_stopping_metric: cosine_sim\n\ntrial_name: foundation_finetune_v1\n</code></pre>"},{"location":"onboarding-model/#5-optional-define-config-setter","title":"5. Optional: Define Config Setter","text":"<p>Sometimes you need to set config values at runtime based on the loaded data.</p> <pre><code>import core.registry as registry\n\n@registry.register_config_setter('my_model')\ndef my_config_setter(experiment_config, raws, df_word):\n    # Set values based on data\n    num_electrodes = sum([len(raw.ch_names) for raw in raws])\n    experiment_config.model_spec.params['input_channels'] = num_electrodes\n    return experiment_config\n</code></pre> <p>Function Signature: <pre><code>from core.config import ExperimentConfig\n\ndef config_setter(\n    experiment_config: ExperimentConfig,\n    raws: list[mne.io.Raw],\n    df_word: pd.DataFrame\n) -&gt; ExperimentConfig\n</code></pre></p> <p>Multiple Config Setters: You can apply multiple setters in sequence: <pre><code># Single setter\nconfig_setter_name: my_model\n\n# Multiple setters (applied in order)\nconfig_setter_name: [set_input_channels, set_embedding_dim, initialize_model]\n</code></pre></p> <p>This is useful for: - Applying task-specific setters from <code>task_specific_config.required_config_setter_names</code> - Following up with model-specific setters - Chaining multiple config transformations</p>"},{"location":"onboarding-model/#examples_2","title":"Examples","text":"<p>Neural Conv (set number of input channels): <pre><code>@registry.register_config_setter('neural_conv')\ndef set_config_input_channels(experiment_config, raws, _df_word):\n    num_electrodes = sum([len(raw.ch_names) for raw in raws])\n    experiment_config.model_spec.params['input_channels'] = num_electrodes\n    return experiment_config\n</code></pre></p> <p>Foundation Model Finetuning (load foundation config and set dimensions): <pre><code>@registry.register_config_setter(\"foundation_model_finetune_mlp\")\ndef foundation_model_mlp_finetune_config_setter(\n    experiment_config, raws, _df_word\n):\n    # Add channel names for preprocessing\n    ch_names = sum([raw.info.ch_names for raw in raws], [])\n    experiment_config.task_config.data_params.preprocessor_params = {\"ch_names\": ch_names}\n\n    # Load foundation model config\n    config_path = os.path.join(\n        experiment_config.model_spec.params[\"model_dir\"],\n        \"experiment_config.yml\"\n    )\n    foundation_config = load_config(config_path)\n\n    # Set dimensions and window width from foundation model\n    experiment_config.model_spec.params[\"foundation_model_config\"] = foundation_config\n    experiment_config.model_spec.params[\"model_dim\"] = foundation_config.vit_config.dim\n    experiment_config.task_config.data_params.window_width = foundation_config.sample_length\n    experiment_config.task_config.data_params.preprocessor_params[\"ecog_data_config\"] = (\n        foundation_config.ecog_data_config\n    )\n\n    return experiment_config\n</code></pre></p>"},{"location":"onboarding-model/#6-import-module","title":"6. Import Module","text":"<p>Your module will be automatically imported! The framework recursively imports all models from the <code>models/</code> directory:</p> <pre><code># Import all models from the models/ directory (recursively imports all subpackages)\nimport_all_from_package(\"models\", recursive=True)\n</code></pre> <p>As long as your model is in <code>models/my_model/</code>, it will be automatically discovered and loaded at runtime.</p> <p>Critical: Make sure you've added the <code>@registry</code> decorators to your functions!</p>"},{"location":"onboarding-model/#7-optional-update-makefile","title":"7. Optional: Update Makefile","text":"<p>Add a convenient make rule for your model:</p> <pre><code>my-model:\n    mkdir -p logs\n    $(CMD) main.py \\\n        --config configs/my_model/config.yml\n</code></pre> <p>Now you can run with: <pre><code>make my-model\n</code></pre></p>"},{"location":"onboarding-model/#8-run-training","title":"8. Run Training","text":"<p>Run your model:</p> <pre><code>make my-model\n</code></pre> <p>Or directly: <pre><code>python main.py --config configs/my_model/config.yml\n</code></pre></p> <p>Results will be saved to: - <code>results/</code> - Performance metrics - <code>checkpoints/</code> - Model checkpoints - <code>event_logs/</code> - TensorBoard logs (if enabled)</p>"},{"location":"onboarding-model/#debugging","title":"Debugging","text":"<p>If you encounter errors:</p> <ol> <li>Check that all <code>@registry</code> decorators are present</li> <li>Verify your module is imported in <code>main.py</code></li> <li>Ensure function names match between config and registered functions</li> <li>Look at logs in <code>logs/</code> for SLURM jobs</li> </ol>"},{"location":"onboarding-model/#complete-working-example","title":"Complete Working Example","text":"<p>See <code>models/example_foundation_model/</code> for a complete, self-contained example demonstrating:</p> <ul> <li>Simple transformer foundation model implementation</li> <li>Both integration patterns (feature extraction + finetuning)</li> <li>Model directory structure with config and checkpoint</li> <li>Full documentation and runnable examples</li> </ul> <p>This example shows exactly how all the pieces fit together for foundation models.</p> <pre><code># Run feature extraction example\npython main.py --config configs/example_foundation_model/feature_extraction.yaml\n\n# Run finetuning example\npython main.py --config configs/example_foundation_model/finetuning.yaml\n</code></pre> <p>See <code>models/example_foundation_model/README.md</code> for details.</p>"},{"location":"onboarding-model/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - Detailed config options and patterns</li> <li>Task Reference - Complete reference for all available tasks</li> <li>Adding a Task - Create custom decoding tasks</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Get started with the podcast benchmark framework in minutes.</p>"},{"location":"quickstart/#setup","title":"Setup","text":"<p>To download data and set up your local virtual environment:</p> <pre><code>./setup.sh\n</code></pre> <p>This will: - Create a Python virtual environment - Install all required dependencies - Download the necessary podcast listening data</p>"},{"location":"quickstart/#training-your-first-model","title":"Training Your First Model","text":"<p>The framework comes with two pre-configured models you can train immediately.</p>"},{"location":"quickstart/#1-neural-convolutional-decoder","title":"1. Neural Convolutional Decoder","text":"<p>This recreates the decoder from Tang et al. 2022, which decodes word embeddings directly from neural data:</p> <pre><code>make neural-conv\n</code></pre>"},{"location":"quickstart/#2-foundation-model-decoder","title":"2. Foundation Model Decoder","text":"<p>This trains a decoder from a foundation model's latent representations to word embeddings:</p> <pre><code>make foundation-model\n</code></pre>"},{"location":"quickstart/#results","title":"Results","text":"<p>Training results will be saved to: - <code>results/</code> - Performance metrics and CSV files - <code>checkpoints/</code> - Saved model checkpoints - <code>event_logs/</code> - TensorBoard logs</p> <p>See Baseline Results for performance benchmarks across all tasks.</p>"},{"location":"quickstart/#configuration","title":"Configuration","text":"<p>To modify data, behavior, or hyperparameters:</p> <p>Edit the relevant configuration file in <code>configs/</code>: - <code>configs/neural_conv_decoder/</code> - Neural convolutional decoder settings - <code>configs/example_foundation_model/</code> - Foundation model decoder settings</p> <p>Model implementations can be found in the <code>models/</code> directory.</p> <p>See Onboarding a New Model for details on configuration options.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Add your own model</li> <li>Create a custom task</li> <li>View all available tasks</li> <li>Compare against baseline results</li> <li>Explore the API</li> </ul>"},{"location":"task-reference/","title":"Task Reference","text":"<p>Complete reference for all available tasks in the podcast benchmark framework.</p>"},{"location":"task-reference/#overview","title":"Overview","text":"<p>Tasks define what you want to decode from neural data. Each task provides a DataFrame with timestamps (<code>start</code>) and targets (<code>target</code>) that serve as training labels for your models.</p> <p>All tasks are located in the <code>tasks/</code> directory and must be registered using the <code>@registry.register_task_data_getter()</code> decorator.</p> <p>For performance benchmarks on each task, see Baseline Results.</p>"},{"location":"task-reference/#task-list","title":"Task List","text":"<ul> <li>word_embedding_decoding_task</li> <li>sentence_onset_task</li> <li>content_noncontent_task</li> <li>pos_task</li> <li>gpt_surprise_task</li> <li>gpt_surprise_multiclass_task</li> <li>volume_level_decoding_task</li> <li>llm_decoding_task</li> </ul>"},{"location":"task-reference/#word_embedding_decoding_task","title":"word_embedding_decoding_task","text":"<p>File: <code>tasks/word_embedding.py</code></p> <p>Description: Decode high-dimensional word embeddings from neural data. Supports GPT-2 XL contextual embeddings, GloVe static embeddings, or custom embeddings.</p> <p>Task Type: Regression (high-dimensional continuous targets)</p> <p>Output: - <code>start</code>: Word start time in seconds - <code>target</code>: Word embedding vector (list or array)</p>"},{"location":"task-reference/#configuration-parameters","title":"Configuration Parameters","text":"<p>Configured via <code>WordEmbeddingConfig</code> in <code>task_specific_config</code>:</p> Parameter Type Default Description <code>embedding_type</code> string <code>\"gpt-2xl\"</code> Embedding type: <code>\"gpt-2xl\"</code>, <code>\"glove\"</code>, or <code>\"arbitrary\"</code> <code>embedding_layer</code> int <code>None</code> GPT-2 layer to extract (0-47 for GPT-2 XL) <code>embedding_pca_dim</code> int <code>None</code> Optional: reduce dimensionality with PCA"},{"location":"task-reference/#embedding-types","title":"Embedding Types","text":"<p><code>gpt-2xl</code>: Contextual embeddings from GPT-2 XL - Requires transcript at <code>{data_root}/stimuli/gpt2-xl/transcript.tsv</code> - Extracts embeddings from specified layer - Handles sub-word tokenization automatically</p> <p><code>glove</code>: Static word embeddings (GloVe) - Requires implementation in <code>tasks/word_embedding.py</code> - Uses lemmatized word forms - Fixed vectors per word type</p> <p><code>arbitrary</code>: Custom embedding implementation - Requires implementation in <code>utils/word_embedding.py</code> - Flexible for any embedding type</p>"},{"location":"task-reference/#word-processing","title":"Word Processing","text":"<p>The task automatically: 1. Groups sub-word tokens into full words using <code>word_idx</code> 2. Normalizes words (lowercase, remove punctuation) 3. Lemmatizes words using NLTK WordNet 4. Aligns embeddings to word boundaries</p>"},{"location":"task-reference/#example-config","title":"Example Config","text":"<pre><code>task_config:\n  task_name: word_embedding_decoding_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    embedding_type: gpt-2xl\n    embedding_layer: 24\n    embedding_pca_dim: 50  # Optional: reduce from 1600 to 50 dims\n</code></pre>"},{"location":"task-reference/#volume_level_decoding_task","title":"volume_level_decoding_task","text":"<p>File: <code>tasks/volume_level.py</code></p> <p>Description: Continuous audio intensity decoding task. Extracts perceptual loudness (in dB) from the podcast audio using Hilbert envelope extraction, low-pass filtering, and optional sliding-window aggregation.</p> <p>Task Type: Regression (continuous targets)</p> <p>Output: - <code>start</code>: Timestamp in seconds - <code>target</code>: Log-amplitude (dB) representing perceptual loudness</p>"},{"location":"task-reference/#configuration-parameters_1","title":"Configuration Parameters","text":"<p>Configured via <code>VolumeLevelConfig</code> in <code>task_specific_config</code>:</p> Parameter Type Default Description <code>audio_path</code> string <code>\"stimuli/podcast.wav\"</code> Path to audio file (relative to <code>data_root</code> or absolute) <code>target_sr</code> int <code>512</code> Target sampling rate for envelope (Hz) <code>audio_sr</code> int <code>44100</code> Expected audio sampling rate (Hz) <code>cutoff_hz</code> float <code>8.0</code> Low-pass filter cutoff frequency (Hz) <code>butter_order</code> int <code>4</code> Butterworth filter order <code>zero_phase</code> bool <code>true</code> Use zero-phase filtering (filtfilt) vs causal (filt) <code>log_eps</code> float auto Epsilon for log compression (auto: peak * 1e-6) <code>allow_resample_audio</code> bool <code>false</code> Allow audio with different sample rate than expected <code>window_size</code> float None Optional: sliding window width in milliseconds <code>hop_size</code> float <code>window_size</code> Optional: sliding window hop size in milliseconds"},{"location":"task-reference/#windowing-behavior","title":"Windowing Behavior","text":"<p>Without windowing (<code>window_size=None</code>): - Returns per-sample dB values - Timestamps are evenly spaced at <code>1/target_sr</code> intervals - Formula: <code>20 * log10(envelope + log_eps)</code></p> <p>With windowing: - Applies sliding RMS windows to the envelope - Converts each RMS window to dB - Timestamps are at window centers - More robust to noise, better aligned with neural integration windows</p>"},{"location":"task-reference/#example-config_1","title":"Example Config","text":"<pre><code>task_config:\n  task_name: volume_level_decoding_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.2\n  task_specific_config:\n    audio_path: stimuli/podcast.wav\n    target_sr: 512\n    cutoff_hz: 8.0\n    window_size: 200.0\n    hop_size: 25.0\n</code></pre>"},{"location":"task-reference/#content_noncontent_task","title":"content_noncontent_task","text":"<p>File: <code>tasks/content_noncontent.py</code></p> <p>Description: Binary classification of content words (nouns, verbs, adjectives, adverbs) vs non-content words (determiners, prepositions, etc.).</p> <p>Task Type: Binary classification</p> <p>Output: - <code>start</code>: Word onset time in seconds - <code>target</code>: <code>1.0</code> for content words, <code>0.0</code> for non-content words</p>"},{"location":"task-reference/#configuration-parameters_2","title":"Configuration Parameters","text":"<p>Configured via <code>ContentNonContentConfig</code>:</p> Parameter Type Default <code>content_noncontent_path</code> string <code>\"processed_data/df_word_onset_with_pos_class.csv\"</code>"},{"location":"task-reference/#example-config_2","title":"Example Config","text":"<pre><code>task_config:\n  task_name: content_noncontent_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    content_noncontent_path: processed_data/df_word_onset_with_pos_class.csv\n</code></pre>"},{"location":"task-reference/#pos_task","title":"pos_task","text":"<p>File: <code>tasks/pos_task.py</code></p> <p>Description: Multi-class part-of-speech classification for words.</p> <p>Task Type: Multi-class classification (5 classes: Noun, Verb, Adjective, Adverb, Other)</p> <p>Output: - <code>start</code>: Word onset time in seconds - <code>target</code>: Class label (0-4)</p>"},{"location":"task-reference/#configuration-parameters_3","title":"Configuration Parameters","text":"<p>Configured via <code>PosTaskConfig</code>:</p> Parameter Type Default <code>pos_path</code> string <code>\"processed_data/df_word_onset_with_pos_class.csv\"</code>"},{"location":"task-reference/#example-config_3","title":"Example Config","text":"<pre><code>task_config:\n  task_name: pos_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    pos_path: processed_data/df_word_onset_with_pos_class.csv\n</code></pre>"},{"location":"task-reference/#sentence_onset_task","title":"sentence_onset_task","text":"<p>File: <code>tasks/sentence_onset.py</code></p> <p>Description: Binary classification for detecting sentence onsets with negative sampling.</p> <p>Task Type: Binary classification</p> <p>Output: - <code>start</code>: Time in seconds - <code>target</code>: <code>1.0</code> for sentence onset, <code>0.0</code> for negative examples</p>"},{"location":"task-reference/#configuration-parameters_4","title":"Configuration Parameters","text":"<p>Configured via <code>SentenceOnsetConfig</code>:</p> Parameter Type Default <code>sentence_csv_path</code> string <code>\"processed_data/all_sentences_podcast.csv\"</code> <code>negatives_per_positive</code> int <code>1</code> <code>negative_margin_s</code> float <code>2.0</code>"},{"location":"task-reference/#example-config_4","title":"Example Config","text":"<pre><code>task_config:\n  task_name: sentence_onset_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    sentence_csv_path: processed_data/all_sentences_podcast.csv\n    negatives_per_positive: 5\n    negative_margin_s: 0.75\n</code></pre>"},{"location":"task-reference/#gpt_surprise_task","title":"gpt_surprise_task","text":"<p>File: <code>tasks/gpt_surprise.py</code></p> <p>Description: Regression task predicting GPT-2 XL surprise values.</p> <p>Task Type: Regression (continuous targets)</p> <p>Output: - <code>start</code>: Word onset time in seconds - <code>target</code>: GPT-2 XL surprise value</p>"},{"location":"task-reference/#configuration-parameters_5","title":"Configuration Parameters","text":"<p>Configured via <code>GptSurpriseConfig</code>:</p> Parameter Type Default <code>content_noncontent_path</code> string <code>\"processed_data/df_word_onset_with_pos_class.csv\"</code>"},{"location":"task-reference/#example-config_5","title":"Example Config","text":"<pre><code>task_config:\n  task_name: gpt_surprise_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    content_noncontent_path: processed_data/df_word_onset_with_pos_class.csv\n</code></pre>"},{"location":"task-reference/#gpt_surprise_multiclass_task","title":"gpt_surprise_multiclass_task","text":"<p>File: <code>tasks/gpt_surprise.py</code></p> <p>Description: Multi-class classification of GPT-2 XL surprise levels.</p> <p>Task Type: Multi-class classification (3 classes: Low, Medium, High surprise)</p> <p>Output: - <code>start</code>: Word onset time in seconds - <code>target</code>: Class label (0-2)</p>"},{"location":"task-reference/#configuration-parameters_6","title":"Configuration Parameters","text":"<p>Configured via <code>GptSurpriseConfig</code>:</p> Parameter Type Default <code>content_noncontent_path</code> string <code>\"processed_data/df_word_onset_with_pos_class.csv\"</code>"},{"location":"task-reference/#example-config_6","title":"Example Config","text":"<pre><code>task_config:\n  task_name: gpt_surprise_multiclass_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    content_noncontent_path: processed_data/df_word_onset_with_pos_class.csv\n</code></pre>"},{"location":"task-reference/#llm_decoding_task","title":"llm_decoding_task","text":"<p>File: <code>tasks/llm_decoding.py</code></p> <p>Description: Language model decoding task that encodes brain data and passes it as a vector input to a language model (GPT-2). The brain encoder transforms neural activity into embeddings that are prepended to the text context with special separator tokens, allowing the model to predict words conditioned on both brain data and linguistic context. This enables direct brain-to-text decoding using pretrained language models.</p> <p>Task Type: Language generation (token-level prediction)</p> <p>Output: - <code>start</code>: Word start time in seconds - <code>end</code>: Word end time in seconds - <code>word</code>: The target word string - <code>prev_input_ids</code>: Token IDs for context window only (max_context tokens) - <code>prev_attention_mask</code>: Attention mask for context tokens - <code>all_input_ids</code>: Token IDs for context + target (max_context + max_target_tokens) - <code>all_attention_mask</code>: Attention mask for all tokens - <code>target</code>: Target token IDs for the word (padded to max_target_tokens, -100 for padding) - <code>target_attention_mask</code>: Attention mask for target tokens</p>"},{"location":"task-reference/#configuration-parameters_7","title":"Configuration Parameters","text":"<p>Configured via <code>LlmDecodingConfig</code> in <code>task_specific_config</code>:</p> Parameter Type Default Description <code>max_context</code> int <code>32</code> Maximum number of context tokens before target word <code>max_target_tokens</code> int <code>16</code> Maximum number of tokens in target word <code>transcript_path</code> string <code>\"data/stimuli/podcast_transcript.csv\"</code> Path to transcript CSV file <code>prepend_space</code> bool <code>true</code> Whether to prepend a space to context windows <code>model_name</code> string <code>\"gpt2\"</code> GPT-2 model variant (gpt2, gpt2-medium, gpt2-large, gpt2-xl) <code>cache_dir</code> string <code>\"./model_cache\"</code> Directory to cache downloaded models <code>input_fields</code> list[str] <code>[\"all_input_ids\", \"all_attention_mask\", \"target_attention_mask\"]</code> DataFrame columns to pass as model inputs <code>required_config_setter_names</code> list[str] <code>[\"llm_decoding_config_setter\"]</code> Config setters to run"},{"location":"task-reference/#model-architecture","title":"Model Architecture","text":"<p>This task requires the <code>GPT2Brain</code> model (<code>language_generation/gpt2_brain.py</code>):</p> <pre><code>Input Flow:\n  Brain Data [batch, channels, timepoints]\n       \u2193\n  Neural Encoder \u2192 Brain Embeddings [batch, embed_dim]\n       \u2193\n  Wrapped: [&lt;brain/&gt;, embeddings, &lt;/brain&gt;]\n       \u2193\n  Concatenated with tokenized context\n       \u2193\n  GPT-2 Language Model (frozen)\n       \u2193\n  Token Predictions\n</code></pre> <p>Key Components: - Neural Encoder: Trainable model that transforms brain data to GPT-2 embedding space - Frozen GPT-2: Pretrained language model provides linguistic knowledge - Brain Tokens: Special tokens (<code>&lt;brain/&gt;</code>, <code>&lt;/brain&gt;</code>) allow the model to distinguish brain input from text - Selective Training: Only the encoder and brain token embeddings are trained</p>"},{"location":"task-reference/#optional-embedding-pre-training","title":"Optional: Embedding Pre-training","text":"<p>For better performance, you can use a two-stage training approach:</p> <p>Stage 1 - Pre-train Encoder (<code>llm_embedding_pretraining_task</code>): - Train the encoder to predict average GPT-2 token embeddings from brain data - Faster training, provides good initialization - Uses simple regression objective (MSE or cosine distance)</p> <p>Stage 2 - Fine-tune for Token Prediction (<code>llm_decoding_task</code>): - Load pre-trained encoder into <code>GPT2Brain</code> - Fine-tune end-to-end for actual token prediction - Better final performance than training from scratch</p> <p>Pre-training Config Example: <pre><code># Stage 1: Pre-train encoder on embeddings\ntask_config:\n  task_name: llm_embedding_pretraining_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    max_context: 32\n    max_target_tokens: 16\n    model_name: gpt2\n    cache_dir: ./model_cache\n\nmodel_spec:\n  constructor_name: pitom_model\n  params:\n    input_channels: 64\n    output_dim: 768  # Must match GPT-2 embedding dimension\n\ntraining_params:\n  losses: [mse]\n  metrics: [cosine_sim]\n</code></pre></p>"},{"location":"task-reference/#example-config_7","title":"Example Config","text":"<pre><code># Stage 2 (or direct training): Full LLM decoding\ntask_config:\n  task_name: llm_decoding_task\n  data_params:\n    data_root: data\n    subject_ids: [1, 2, 3]\n    window_width: 0.625\n  task_specific_config:\n    max_context: 32\n    max_target_tokens: 16\n    transcript_path: data/stimuli/podcast_transcript.csv\n    model_name: gpt2\n    cache_dir: ./model_cache\n\nmodel_spec:\n  constructor_name: gpt2_brain\n  params:\n    freeze_lm: true\n    cache_dir: ./model_cache\n  sub_models:\n    encoder_model:\n      constructor_name: pitom_model\n      params:\n        input_channels: 64\n        output_dim: 768\n\ntraining_params:\n  losses: [cross_entropy]\n  metrics: [accuracy, perplexity]\n  learning_rate: 0.0001\n</code></pre>"},{"location":"task-reference/#see-also","title":"See Also","text":"<ul> <li>Baseline Results: Performance benchmarks for all tasks</li> <li>Configuration Guide: Full configuration reference</li> <li>Adding a Task: Step-by-step guide for implementing tasks</li> <li>API Reference: Detailed API documentation</li> </ul>"}]}